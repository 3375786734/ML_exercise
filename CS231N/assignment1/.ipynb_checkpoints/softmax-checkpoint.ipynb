{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (49000, 3073)\n",
      "Training labels shape:  (49000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev shape (500, 3073)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train,y_train,X_val,y_val,X_test,y_test,X_dev,y_dev = get_CIFAR10_data()\n",
    "'''\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "X_train,X_test = X_train - np.mean(X_train,axis = 0),X_test -np.mean(X_test,axis = 0)\n",
    "\n",
    "num_train,num_dev = X_train.shape[0],500\n",
    "\n",
    "dev_idx = np.random.choice(num_train,num_dev,replace = False)\n",
    "X_dev,y_dev = X_train[dev_idx],y_train[dev_idx]\n",
    "X_train,X_test,X_dev = X_train.reshape(X_train.shape[0],-1),X_test.reshape(X_test.shape[0],-1),X_dev.reshape(X_dev.shape[0],-1)\n",
    "\n",
    "\n",
    "\n",
    "X_train,X_test,X_dev = np.hstack([X_train,np.ones((X_train.shape[0],0))]),np.hstack([X_test,np.ones((X_test.shape[0],1))]),np.hstack([X_dev,np.ones((X_dev.shape[0],1))])\n",
    "'''\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev shape',X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.383355\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.\n",
    "\n",
    "**Your answer:** since we initialize it randomly,hence there is Probability of $\\log0.1$ to get true label ,hence the averege loss is about $-\\log0.1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 3.326603 analytic: 3.326603, relative error: 1.488465e-08\n",
      "numerical: -4.041257 analytic: -4.041257, relative error: 8.144486e-09\n",
      "numerical: -0.809490 analytic: -0.809490, relative error: 7.022891e-09\n",
      "numerical: -0.315281 analytic: -0.315281, relative error: 7.902662e-08\n",
      "numerical: 0.902247 analytic: 0.902247, relative error: 7.090927e-08\n",
      "numerical: 0.520326 analytic: 0.520326, relative error: 9.670622e-08\n",
      "numerical: -1.500409 analytic: -1.500409, relative error: 1.935725e-08\n",
      "numerical: -2.547266 analytic: -2.547266, relative error: 6.464758e-09\n",
      "numerical: 1.741382 analytic: 1.741381, relative error: 5.858341e-08\n",
      "numerical: -0.066457 analytic: -0.066457, relative error: 5.993561e-07\n",
      "numerical: 0.967276 analytic: 0.961830, relative error: 2.823488e-03\n",
      "numerical: 3.251817 analytic: 3.247914, relative error: 6.004601e-04\n",
      "numerical: -0.038539 analytic: -0.038192, relative error: 4.517180e-03\n",
      "numerical: -6.311134 analytic: -6.310004, relative error: 8.952568e-05\n",
      "numerical: 1.404299 analytic: 1.405329, relative error: 3.667538e-04\n",
      "numerical: -3.206401 analytic: -3.207228, relative error: 1.290168e-04\n",
      "numerical: -5.832257 analytic: -5.830610, relative error: 1.412277e-04\n",
      "numerical: 0.576214 analytic: 0.579955, relative error: 3.236088e-03\n",
      "numerical: 1.501674 analytic: 1.500095, relative error: 5.258143e-04\n",
      "numerical: -1.716071 analytic: -1.715246, relative error: 2.405791e-04\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.383355e+00 computed in 0.344846s\n",
      "vectorized loss: 2.383355e+00 computed in 0.036798s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized,grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 216.975210\n",
      "iteration 100 / 2000: loss 187.305354\n",
      "iteration 200 / 2000: loss 162.590286\n",
      "iteration 300 / 2000: loss 141.568124\n",
      "iteration 400 / 2000: loss 122.910509\n",
      "iteration 500 / 2000: loss 106.824614\n",
      "iteration 600 / 2000: loss 92.921588\n",
      "iteration 700 / 2000: loss 80.927914\n",
      "iteration 800 / 2000: loss 70.549546\n",
      "iteration 900 / 2000: loss 61.538772\n",
      "iteration 1000 / 2000: loss 53.614532\n",
      "iteration 1100 / 2000: loss 46.825575\n",
      "iteration 1200 / 2000: loss 40.912193\n",
      "iteration 1300 / 2000: loss 35.707269\n",
      "iteration 1400 / 2000: loss 31.490198\n",
      "iteration 1500 / 2000: loss 27.416187\n",
      "iteration 1600 / 2000: loss 24.126305\n",
      "iteration 1700 / 2000: loss 21.096550\n",
      "iteration 1800 / 2000: loss 18.573606\n",
      "iteration 1900 / 2000: loss 16.396578\n",
      "iteration 0 / 2000: loss 253.438786\n",
      "iteration 100 / 2000: loss 214.894960\n",
      "iteration 200 / 2000: loss 182.790785\n",
      "iteration 300 / 2000: loss 155.882928\n",
      "iteration 400 / 2000: loss 132.553142\n",
      "iteration 500 / 2000: loss 113.086437\n",
      "iteration 600 / 2000: loss 96.458107\n",
      "iteration 700 / 2000: loss 82.501009\n",
      "iteration 800 / 2000: loss 70.354903\n",
      "iteration 900 / 2000: loss 60.206632\n",
      "iteration 1000 / 2000: loss 51.550866\n",
      "iteration 1100 / 2000: loss 44.193328\n",
      "iteration 1200 / 2000: loss 37.840620\n",
      "iteration 1300 / 2000: loss 32.423917\n",
      "iteration 1400 / 2000: loss 27.892596\n",
      "iteration 1500 / 2000: loss 24.073371\n",
      "iteration 1600 / 2000: loss 20.795142\n",
      "iteration 1700 / 2000: loss 18.041161\n",
      "iteration 1800 / 2000: loss 15.595682\n",
      "iteration 1900 / 2000: loss 13.600984\n",
      "iteration 0 / 2000: loss 284.384449\n",
      "iteration 100 / 2000: loss 236.188526\n",
      "iteration 200 / 2000: loss 197.001083\n",
      "iteration 300 / 2000: loss 164.317601\n",
      "iteration 400 / 2000: loss 137.444089\n",
      "iteration 500 / 2000: loss 115.047343\n",
      "iteration 600 / 2000: loss 96.286565\n",
      "iteration 700 / 2000: loss 80.548161\n",
      "iteration 800 / 2000: loss 67.485541\n",
      "iteration 900 / 2000: loss 56.665352\n",
      "iteration 1000 / 2000: loss 47.707143\n",
      "iteration 1100 / 2000: loss 39.950155\n",
      "iteration 1200 / 2000: loss 33.636958\n",
      "iteration 1300 / 2000: loss 28.537583\n",
      "iteration 1400 / 2000: loss 24.081825\n",
      "iteration 1500 / 2000: loss 20.444992\n",
      "iteration 1600 / 2000: loss 17.294456\n",
      "iteration 1700 / 2000: loss 14.847363\n",
      "iteration 1800 / 2000: loss 12.696725\n",
      "iteration 1900 / 2000: loss 11.025529\n",
      "iteration 0 / 2000: loss 312.480939\n",
      "iteration 100 / 2000: loss 254.427189\n",
      "iteration 200 / 2000: loss 208.339789\n",
      "iteration 300 / 2000: loss 170.325104\n",
      "iteration 400 / 2000: loss 139.580312\n",
      "iteration 500 / 2000: loss 114.314128\n",
      "iteration 600 / 2000: loss 93.736475\n",
      "iteration 700 / 2000: loss 76.997446\n",
      "iteration 800 / 2000: loss 63.344733\n",
      "iteration 900 / 2000: loss 52.055543\n",
      "iteration 1000 / 2000: loss 42.898508\n",
      "iteration 1100 / 2000: loss 35.592611\n",
      "iteration 1200 / 2000: loss 29.396834\n",
      "iteration 1300 / 2000: loss 24.376231\n",
      "iteration 1400 / 2000: loss 20.253053\n",
      "iteration 1500 / 2000: loss 16.936362\n",
      "iteration 1600 / 2000: loss 14.274189\n",
      "iteration 1700 / 2000: loss 12.113831\n",
      "iteration 1800 / 2000: loss 10.310530\n",
      "iteration 1900 / 2000: loss 8.724243\n",
      "iteration 0 / 2000: loss 338.823968\n",
      "iteration 100 / 2000: loss 270.377402\n",
      "iteration 200 / 2000: loss 217.127407\n",
      "iteration 300 / 2000: loss 174.036596\n",
      "iteration 400 / 2000: loss 139.888154\n",
      "iteration 500 / 2000: loss 112.256294\n",
      "iteration 600 / 2000: loss 90.312472\n",
      "iteration 700 / 2000: loss 72.763483\n",
      "iteration 800 / 2000: loss 58.742397\n",
      "iteration 900 / 2000: loss 47.419687\n",
      "iteration 1000 / 2000: loss 38.434482\n",
      "iteration 1100 / 2000: loss 31.157086\n",
      "iteration 1200 / 2000: loss 25.328034\n",
      "iteration 1300 / 2000: loss 20.725699\n",
      "iteration 1400 / 2000: loss 17.059796\n",
      "iteration 1500 / 2000: loss 13.995802\n",
      "iteration 1600 / 2000: loss 11.674636\n",
      "iteration 1700 / 2000: loss 9.728869\n",
      "iteration 1800 / 2000: loss 8.279059\n",
      "iteration 1900 / 2000: loss 7.024175\n",
      "iteration 0 / 2000: loss 368.971860\n",
      "iteration 100 / 2000: loss 289.231625\n",
      "iteration 200 / 2000: loss 226.906626\n",
      "iteration 300 / 2000: loss 178.622477\n",
      "iteration 400 / 2000: loss 140.631228\n",
      "iteration 500 / 2000: loss 110.911143\n",
      "iteration 600 / 2000: loss 87.345359\n",
      "iteration 700 / 2000: loss 69.023146\n",
      "iteration 800 / 2000: loss 54.772490\n",
      "iteration 900 / 2000: loss 43.349025\n",
      "iteration 1000 / 2000: loss 34.540323\n",
      "iteration 1100 / 2000: loss 27.431236\n",
      "iteration 1200 / 2000: loss 22.131655\n",
      "iteration 1300 / 2000: loss 17.825272\n",
      "iteration 1400 / 2000: loss 14.349078\n",
      "iteration 1500 / 2000: loss 11.775680\n",
      "iteration 1600 / 2000: loss 9.646554\n",
      "iteration 1700 / 2000: loss 8.083487\n",
      "iteration 1800 / 2000: loss 6.755399\n",
      "iteration 1900 / 2000: loss 5.789689\n",
      "iteration 0 / 2000: loss 407.093506\n",
      "iteration 100 / 2000: loss 312.238936\n",
      "iteration 200 / 2000: loss 240.639184\n",
      "iteration 300 / 2000: loss 185.486249\n",
      "iteration 400 / 2000: loss 143.030061\n",
      "iteration 500 / 2000: loss 110.556233\n",
      "iteration 600 / 2000: loss 85.477151\n",
      "iteration 700 / 2000: loss 66.325074\n",
      "iteration 800 / 2000: loss 51.560190\n",
      "iteration 900 / 2000: loss 40.010470\n",
      "iteration 1000 / 2000: loss 31.304620\n",
      "iteration 1100 / 2000: loss 24.603710\n",
      "iteration 1200 / 2000: loss 19.426548\n",
      "iteration 1300 / 2000: loss 15.387996\n",
      "iteration 1400 / 2000: loss 12.333980\n",
      "iteration 1500 / 2000: loss 9.997756\n",
      "iteration 1600 / 2000: loss 8.118515\n",
      "iteration 1700 / 2000: loss 6.731846\n",
      "iteration 1800 / 2000: loss 5.748193\n",
      "iteration 1900 / 2000: loss 4.810913\n",
      "iteration 0 / 2000: loss 1454.380109\n",
      "iteration 100 / 2000: loss 566.323181\n",
      "iteration 200 / 2000: loss 221.367339\n",
      "iteration 300 / 2000: loss 87.343547\n",
      "iteration 400 / 2000: loss 35.251722\n",
      "iteration 500 / 2000: loss 15.021160\n",
      "iteration 600 / 2000: loss 7.187270\n",
      "iteration 700 / 2000: loss 4.096871\n",
      "iteration 800 / 2000: loss 2.962152\n",
      "iteration 900 / 2000: loss 2.437336\n",
      "iteration 1000 / 2000: loss 2.274667\n",
      "iteration 1100 / 2000: loss 2.198334\n",
      "iteration 1200 / 2000: loss 2.158856\n",
      "iteration 1300 / 2000: loss 2.186834\n",
      "iteration 1400 / 2000: loss 2.174710\n",
      "iteration 1500 / 2000: loss 2.200459\n",
      "iteration 1600 / 2000: loss 2.196946\n",
      "iteration 1700 / 2000: loss 2.148585\n",
      "iteration 1800 / 2000: loss 2.178417\n",
      "iteration 1900 / 2000: loss 2.211493\n",
      "iteration 0 / 2000: loss 1491.173043\n",
      "iteration 100 / 2000: loss 569.133852\n",
      "iteration 200 / 2000: loss 218.047551\n",
      "iteration 300 / 2000: loss 84.430845\n",
      "iteration 400 / 2000: loss 33.534550\n",
      "iteration 500 / 2000: loss 14.132020\n",
      "iteration 600 / 2000: loss 6.726414\n",
      "iteration 700 / 2000: loss 3.921057\n",
      "iteration 800 / 2000: loss 2.857502\n",
      "iteration 900 / 2000: loss 2.403209\n",
      "iteration 1000 / 2000: loss 2.328947\n",
      "iteration 1100 / 2000: loss 2.216584\n",
      "iteration 1200 / 2000: loss 2.151046\n",
      "iteration 1300 / 2000: loss 2.137226\n",
      "iteration 1400 / 2000: loss 2.195602\n",
      "iteration 1500 / 2000: loss 2.166844\n",
      "iteration 1600 / 2000: loss 2.207024\n",
      "iteration 1700 / 2000: loss 2.193652\n",
      "iteration 1800 / 2000: loss 2.167216\n",
      "iteration 1900 / 2000: loss 2.151365\n",
      "iteration 0 / 2000: loss 1514.842465\n",
      "iteration 100 / 2000: loss 566.553200\n",
      "iteration 200 / 2000: loss 212.907831\n",
      "iteration 300 / 2000: loss 80.790239\n",
      "iteration 400 / 2000: loss 31.602624\n",
      "iteration 500 / 2000: loss 13.122440\n",
      "iteration 600 / 2000: loss 6.281280\n",
      "iteration 700 / 2000: loss 3.688550\n",
      "iteration 800 / 2000: loss 2.790952\n",
      "iteration 900 / 2000: loss 2.383465\n",
      "iteration 1000 / 2000: loss 2.311073\n",
      "iteration 1100 / 2000: loss 2.218256\n",
      "iteration 1200 / 2000: loss 2.181700\n",
      "iteration 1300 / 2000: loss 2.165915\n",
      "iteration 1400 / 2000: loss 2.198441\n",
      "iteration 1500 / 2000: loss 2.185358\n",
      "iteration 1600 / 2000: loss 2.180317\n",
      "iteration 1700 / 2000: loss 2.166867\n",
      "iteration 1800 / 2000: loss 2.137173\n",
      "iteration 1900 / 2000: loss 2.203841\n",
      "iteration 0 / 2000: loss 1556.633634\n",
      "iteration 100 / 2000: loss 570.755357\n",
      "iteration 200 / 2000: loss 210.118437\n",
      "iteration 300 / 2000: loss 78.297742\n",
      "iteration 400 / 2000: loss 29.949412\n",
      "iteration 500 / 2000: loss 12.401407\n",
      "iteration 600 / 2000: loss 5.921543\n",
      "iteration 700 / 2000: loss 3.502998\n",
      "iteration 800 / 2000: loss 2.646763\n",
      "iteration 900 / 2000: loss 2.395291\n",
      "iteration 1000 / 2000: loss 2.265513\n",
      "iteration 1100 / 2000: loss 2.196564\n",
      "iteration 1200 / 2000: loss 2.164486\n",
      "iteration 1300 / 2000: loss 2.183778\n",
      "iteration 1400 / 2000: loss 2.180591\n",
      "iteration 1500 / 2000: loss 2.122423\n",
      "iteration 1600 / 2000: loss 2.182744\n",
      "iteration 1700 / 2000: loss 2.084725\n",
      "iteration 1800 / 2000: loss 2.176188\n",
      "iteration 1900 / 2000: loss 2.223187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 1570.003509\n",
      "iteration 100 / 2000: loss 563.702861\n",
      "iteration 200 / 2000: loss 203.492592\n",
      "iteration 300 / 2000: loss 74.391062\n",
      "iteration 400 / 2000: loss 28.125181\n",
      "iteration 500 / 2000: loss 11.480474\n",
      "iteration 600 / 2000: loss 5.559016\n",
      "iteration 700 / 2000: loss 3.404070\n",
      "iteration 800 / 2000: loss 2.632547\n",
      "iteration 900 / 2000: loss 2.360592\n",
      "iteration 1000 / 2000: loss 2.159686\n",
      "iteration 1100 / 2000: loss 2.238691\n",
      "iteration 1200 / 2000: loss 2.169043\n",
      "iteration 1300 / 2000: loss 2.162754\n",
      "iteration 1400 / 2000: loss 2.176294\n",
      "iteration 1500 / 2000: loss 2.166151\n",
      "iteration 1600 / 2000: loss 2.134710\n",
      "iteration 1700 / 2000: loss 2.196301\n",
      "iteration 1800 / 2000: loss 2.141264\n",
      "iteration 1900 / 2000: loss 2.134809\n",
      "iteration 0 / 2000: loss 1600.881272\n",
      "iteration 100 / 2000: loss 563.630927\n",
      "iteration 200 / 2000: loss 199.534980\n",
      "iteration 300 / 2000: loss 71.680280\n",
      "iteration 400 / 2000: loss 26.621551\n",
      "iteration 500 / 2000: loss 10.708000\n",
      "iteration 600 / 2000: loss 5.156208\n",
      "iteration 700 / 2000: loss 3.241146\n",
      "iteration 800 / 2000: loss 2.546626\n",
      "iteration 900 / 2000: loss 2.255002\n",
      "iteration 1000 / 2000: loss 2.183239\n",
      "iteration 1100 / 2000: loss 2.186950\n",
      "iteration 1200 / 2000: loss 2.162881\n",
      "iteration 1300 / 2000: loss 2.188282\n",
      "iteration 1400 / 2000: loss 2.190391\n",
      "iteration 1500 / 2000: loss 2.117330\n",
      "iteration 1600 / 2000: loss 2.199696\n",
      "iteration 1700 / 2000: loss 2.256607\n",
      "iteration 1800 / 2000: loss 2.149744\n",
      "iteration 1900 / 2000: loss 2.172949\n",
      "iteration 0 / 2000: loss 1624.433388\n",
      "iteration 100 / 2000: loss 560.887589\n",
      "iteration 200 / 2000: loss 194.641745\n",
      "iteration 300 / 2000: loss 68.493632\n",
      "iteration 400 / 2000: loss 24.975601\n",
      "iteration 500 / 2000: loss 10.008940\n",
      "iteration 600 / 2000: loss 4.933871\n",
      "iteration 700 / 2000: loss 3.035158\n",
      "iteration 800 / 2000: loss 2.495774\n",
      "iteration 900 / 2000: loss 2.285965\n",
      "iteration 1000 / 2000: loss 2.212131\n",
      "iteration 1100 / 2000: loss 2.178626\n",
      "iteration 1200 / 2000: loss 2.197409\n",
      "iteration 1300 / 2000: loss 2.212944\n",
      "iteration 1400 / 2000: loss 2.262255\n",
      "iteration 1500 / 2000: loss 2.188386\n",
      "iteration 1600 / 2000: loss 2.199773\n",
      "iteration 1700 / 2000: loss 2.187583\n",
      "iteration 1800 / 2000: loss 2.209794\n",
      "iteration 1900 / 2000: loss 2.174435\n",
      "iteration 0 / 2000: loss 221.390445\n",
      "iteration 100 / 2000: loss 185.307364\n",
      "iteration 200 / 2000: loss 156.222205\n",
      "iteration 300 / 2000: loss 132.078373\n",
      "iteration 400 / 2000: loss 111.628155\n",
      "iteration 500 / 2000: loss 94.449208\n",
      "iteration 600 / 2000: loss 79.983639\n",
      "iteration 700 / 2000: loss 67.766633\n",
      "iteration 800 / 2000: loss 57.690390\n",
      "iteration 900 / 2000: loss 48.827987\n",
      "iteration 1000 / 2000: loss 41.541524\n",
      "iteration 1100 / 2000: loss 35.516475\n",
      "iteration 1200 / 2000: loss 30.210208\n",
      "iteration 1300 / 2000: loss 25.809879\n",
      "iteration 1400 / 2000: loss 22.124168\n",
      "iteration 1500 / 2000: loss 18.855508\n",
      "iteration 1600 / 2000: loss 16.291731\n",
      "iteration 1700 / 2000: loss 14.079627\n",
      "iteration 1800 / 2000: loss 12.209091\n",
      "iteration 1900 / 2000: loss 10.662043\n",
      "iteration 0 / 2000: loss 249.125036\n",
      "iteration 100 / 2000: loss 204.389848\n",
      "iteration 200 / 2000: loss 168.212909\n",
      "iteration 300 / 2000: loss 138.994441\n",
      "iteration 400 / 2000: loss 114.724155\n",
      "iteration 500 / 2000: loss 94.884160\n",
      "iteration 600 / 2000: loss 78.409066\n",
      "iteration 700 / 2000: loss 64.964975\n",
      "iteration 800 / 2000: loss 53.909539\n",
      "iteration 900 / 2000: loss 44.652903\n",
      "iteration 1000 / 2000: loss 37.265320\n",
      "iteration 1100 / 2000: loss 30.965107\n",
      "iteration 1200 / 2000: loss 25.908520\n",
      "iteration 1300 / 2000: loss 21.593006\n",
      "iteration 1400 / 2000: loss 18.239369\n",
      "iteration 1500 / 2000: loss 15.439758\n",
      "iteration 1600 / 2000: loss 13.017015\n",
      "iteration 1700 / 2000: loss 11.050425\n",
      "iteration 1800 / 2000: loss 9.443613\n",
      "iteration 1900 / 2000: loss 8.146929\n",
      "iteration 0 / 2000: loss 284.156330\n",
      "iteration 100 / 2000: loss 227.402692\n",
      "iteration 200 / 2000: loss 182.997546\n",
      "iteration 300 / 2000: loss 147.359744\n",
      "iteration 400 / 2000: loss 118.986253\n",
      "iteration 500 / 2000: loss 95.908988\n",
      "iteration 600 / 2000: loss 77.472593\n",
      "iteration 700 / 2000: loss 62.724697\n",
      "iteration 800 / 2000: loss 50.879396\n",
      "iteration 900 / 2000: loss 41.271362\n",
      "iteration 1000 / 2000: loss 33.610869\n",
      "iteration 1100 / 2000: loss 27.404789\n",
      "iteration 1200 / 2000: loss 22.315674\n",
      "iteration 1300 / 2000: loss 18.377705\n",
      "iteration 1400 / 2000: loss 15.270083\n",
      "iteration 1500 / 2000: loss 12.553581\n",
      "iteration 1600 / 2000: loss 10.483538\n",
      "iteration 1700 / 2000: loss 8.772975\n",
      "iteration 1800 / 2000: loss 7.536855\n",
      "iteration 1900 / 2000: loss 6.621672\n",
      "iteration 0 / 2000: loss 314.775862\n",
      "iteration 100 / 2000: loss 245.476406\n",
      "iteration 200 / 2000: loss 192.884289\n",
      "iteration 300 / 2000: loss 151.671958\n",
      "iteration 400 / 2000: loss 119.437883\n",
      "iteration 500 / 2000: loss 94.066569\n",
      "iteration 600 / 2000: loss 74.354713\n",
      "iteration 700 / 2000: loss 58.691727\n",
      "iteration 800 / 2000: loss 46.525934\n",
      "iteration 900 / 2000: loss 36.894760\n",
      "iteration 1000 / 2000: loss 29.479777\n",
      "iteration 1100 / 2000: loss 23.598815\n",
      "iteration 1200 / 2000: loss 19.011797\n",
      "iteration 1300 / 2000: loss 15.265036\n",
      "iteration 1400 / 2000: loss 12.391682\n",
      "iteration 1500 / 2000: loss 10.193251\n",
      "iteration 1600 / 2000: loss 8.438503\n",
      "iteration 1700 / 2000: loss 7.034319\n",
      "iteration 1800 / 2000: loss 5.985365\n",
      "iteration 1900 / 2000: loss 5.221451\n",
      "iteration 0 / 2000: loss 335.809123\n",
      "iteration 100 / 2000: loss 257.250829\n",
      "iteration 200 / 2000: loss 197.112752\n",
      "iteration 300 / 2000: loss 151.532194\n",
      "iteration 400 / 2000: loss 116.508553\n",
      "iteration 500 / 2000: loss 89.683395\n",
      "iteration 600 / 2000: loss 69.263394\n",
      "iteration 700 / 2000: loss 53.606982\n",
      "iteration 800 / 2000: loss 41.513328\n",
      "iteration 900 / 2000: loss 32.263706\n",
      "iteration 1000 / 2000: loss 25.270164\n",
      "iteration 1100 / 2000: loss 19.782888\n",
      "iteration 1200 / 2000: loss 15.658579\n",
      "iteration 1300 / 2000: loss 12.501781\n",
      "iteration 1400 / 2000: loss 10.049176\n",
      "iteration 1500 / 2000: loss 8.258210\n",
      "iteration 1600 / 2000: loss 6.710317\n",
      "iteration 1700 / 2000: loss 5.683148\n",
      "iteration 1800 / 2000: loss 4.759056\n",
      "iteration 1900 / 2000: loss 4.118678\n",
      "iteration 0 / 2000: loss 375.860429\n",
      "iteration 100 / 2000: loss 280.467642\n",
      "iteration 200 / 2000: loss 210.033639\n",
      "iteration 300 / 2000: loss 157.480401\n",
      "iteration 400 / 2000: loss 118.104471\n",
      "iteration 500 / 2000: loss 89.011909\n",
      "iteration 600 / 2000: loss 67.198536\n",
      "iteration 700 / 2000: loss 50.674651\n",
      "iteration 800 / 2000: loss 38.424934\n",
      "iteration 900 / 2000: loss 29.138160\n",
      "iteration 1000 / 2000: loss 22.519967\n",
      "iteration 1100 / 2000: loss 17.293731\n",
      "iteration 1200 / 2000: loss 13.489016\n",
      "iteration 1300 / 2000: loss 10.635853\n",
      "iteration 1400 / 2000: loss 8.391452\n",
      "iteration 1500 / 2000: loss 6.804518\n",
      "iteration 1600 / 2000: loss 5.641908\n",
      "iteration 1700 / 2000: loss 4.707672\n",
      "iteration 1800 / 2000: loss 4.061266\n",
      "iteration 1900 / 2000: loss 3.603128\n",
      "iteration 0 / 2000: loss 410.651957\n",
      "iteration 100 / 2000: loss 298.019865\n",
      "iteration 200 / 2000: loss 217.686195\n",
      "iteration 300 / 2000: loss 159.549961\n",
      "iteration 400 / 2000: loss 117.062634\n",
      "iteration 500 / 2000: loss 85.881101\n",
      "iteration 600 / 2000: loss 63.329448\n",
      "iteration 700 / 2000: loss 46.845067\n",
      "iteration 800 / 2000: loss 34.767605\n",
      "iteration 900 / 2000: loss 25.873214\n",
      "iteration 1000 / 2000: loss 19.540570\n",
      "iteration 1100 / 2000: loss 14.753753\n",
      "iteration 1200 / 2000: loss 11.375518\n",
      "iteration 1300 / 2000: loss 8.803891\n",
      "iteration 1400 / 2000: loss 7.022032\n",
      "iteration 1500 / 2000: loss 5.640859\n",
      "iteration 1600 / 2000: loss 4.728709\n",
      "iteration 1700 / 2000: loss 3.986634\n",
      "iteration 1800 / 2000: loss 3.416380\n",
      "iteration 1900 / 2000: loss 3.076749\n",
      "iteration 0 / 2000: loss 1426.217127\n",
      "iteration 100 / 2000: loss 459.869439\n",
      "iteration 200 / 2000: loss 149.439220\n",
      "iteration 300 / 2000: loss 49.557943\n",
      "iteration 400 / 2000: loss 17.422250\n",
      "iteration 500 / 2000: loss 7.131305\n",
      "iteration 600 / 2000: loss 3.778512\n",
      "iteration 700 / 2000: loss 2.645890\n",
      "iteration 800 / 2000: loss 2.350640\n",
      "iteration 900 / 2000: loss 2.280175\n",
      "iteration 1000 / 2000: loss 2.257010\n",
      "iteration 1100 / 2000: loss 2.132966\n",
      "iteration 1200 / 2000: loss 2.172852\n",
      "iteration 1300 / 2000: loss 2.131339\n",
      "iteration 1400 / 2000: loss 2.150369\n",
      "iteration 1500 / 2000: loss 2.202162\n",
      "iteration 1600 / 2000: loss 2.166020\n",
      "iteration 1700 / 2000: loss 2.190517\n",
      "iteration 1800 / 2000: loss 2.157574\n",
      "iteration 1900 / 2000: loss 2.199968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 1483.260047\n",
      "iteration 100 / 2000: loss 466.937515\n",
      "iteration 200 / 2000: loss 148.085168\n",
      "iteration 300 / 2000: loss 47.993685\n",
      "iteration 400 / 2000: loss 16.574567\n",
      "iteration 500 / 2000: loss 6.720242\n",
      "iteration 600 / 2000: loss 3.610765\n",
      "iteration 700 / 2000: loss 2.557780\n",
      "iteration 800 / 2000: loss 2.307079\n",
      "iteration 900 / 2000: loss 2.188325\n",
      "iteration 1000 / 2000: loss 2.167349\n",
      "iteration 1100 / 2000: loss 2.168198\n",
      "iteration 1200 / 2000: loss 2.184752\n",
      "iteration 1300 / 2000: loss 2.156858\n",
      "iteration 1400 / 2000: loss 2.161051\n",
      "iteration 1500 / 2000: loss 2.150600\n",
      "iteration 1600 / 2000: loss 2.204119\n",
      "iteration 1700 / 2000: loss 2.105784\n",
      "iteration 1800 / 2000: loss 2.148381\n",
      "iteration 1900 / 2000: loss 2.173903\n",
      "iteration 0 / 2000: loss 1536.807821\n",
      "iteration 100 / 2000: loss 472.379831\n",
      "iteration 200 / 2000: loss 146.140962\n",
      "iteration 300 / 2000: loss 46.330364\n",
      "iteration 400 / 2000: loss 15.706190\n",
      "iteration 500 / 2000: loss 6.338983\n",
      "iteration 600 / 2000: loss 3.437589\n",
      "iteration 700 / 2000: loss 2.586812\n",
      "iteration 800 / 2000: loss 2.275041\n",
      "iteration 900 / 2000: loss 2.185537\n",
      "iteration 1000 / 2000: loss 2.223726\n",
      "iteration 1100 / 2000: loss 2.198802\n",
      "iteration 1200 / 2000: loss 2.188913\n",
      "iteration 1300 / 2000: loss 2.190293\n",
      "iteration 1400 / 2000: loss 2.158779\n",
      "iteration 1500 / 2000: loss 2.202032\n",
      "iteration 1600 / 2000: loss 2.162751\n",
      "iteration 1700 / 2000: loss 2.158482\n",
      "iteration 1800 / 2000: loss 2.169569\n",
      "iteration 1900 / 2000: loss 2.134095\n",
      "iteration 0 / 2000: loss 1546.576670\n",
      "iteration 100 / 2000: loss 463.634256\n",
      "iteration 200 / 2000: loss 140.164553\n",
      "iteration 300 / 2000: loss 43.456360\n",
      "iteration 400 / 2000: loss 14.598995\n",
      "iteration 500 / 2000: loss 5.865598\n",
      "iteration 600 / 2000: loss 3.293828\n",
      "iteration 700 / 2000: loss 2.459875\n",
      "iteration 800 / 2000: loss 2.306075\n",
      "iteration 900 / 2000: loss 2.214048\n",
      "iteration 1000 / 2000: loss 2.206755\n",
      "iteration 1100 / 2000: loss 2.191371\n",
      "iteration 1200 / 2000: loss 2.169518\n",
      "iteration 1300 / 2000: loss 2.207261\n",
      "iteration 1400 / 2000: loss 2.191690\n",
      "iteration 1500 / 2000: loss 2.147530\n",
      "iteration 1600 / 2000: loss 2.204917\n",
      "iteration 1700 / 2000: loss 2.121534\n",
      "iteration 1800 / 2000: loss 2.141104\n",
      "iteration 1900 / 2000: loss 2.167243\n",
      "iteration 0 / 2000: loss 1570.702728\n",
      "iteration 100 / 2000: loss 459.314105\n",
      "iteration 200 / 2000: loss 135.638849\n",
      "iteration 300 / 2000: loss 41.134112\n",
      "iteration 400 / 2000: loss 13.596516\n",
      "iteration 500 / 2000: loss 5.577434\n",
      "iteration 600 / 2000: loss 3.148430\n",
      "iteration 700 / 2000: loss 2.492826\n",
      "iteration 800 / 2000: loss 2.251759\n",
      "iteration 900 / 2000: loss 2.211963\n",
      "iteration 1000 / 2000: loss 2.155159\n",
      "iteration 1100 / 2000: loss 2.175258\n",
      "iteration 1200 / 2000: loss 2.220685\n",
      "iteration 1300 / 2000: loss 2.253548\n",
      "iteration 1400 / 2000: loss 2.184288\n",
      "iteration 1500 / 2000: loss 2.132387\n",
      "iteration 1600 / 2000: loss 2.167928\n",
      "iteration 1700 / 2000: loss 2.174023\n",
      "iteration 1800 / 2000: loss 2.191823\n",
      "iteration 1900 / 2000: loss 2.181485\n",
      "iteration 0 / 2000: loss 1599.479774\n",
      "iteration 100 / 2000: loss 457.087927\n",
      "iteration 200 / 2000: loss 131.963805\n",
      "iteration 300 / 2000: loss 39.163038\n",
      "iteration 400 / 2000: loss 12.730289\n",
      "iteration 500 / 2000: loss 5.174742\n",
      "iteration 600 / 2000: loss 3.055196\n",
      "iteration 700 / 2000: loss 2.412252\n",
      "iteration 800 / 2000: loss 2.288275\n",
      "iteration 900 / 2000: loss 2.208219\n",
      "iteration 1000 / 2000: loss 2.200716\n",
      "iteration 1100 / 2000: loss 2.175299\n",
      "iteration 1200 / 2000: loss 2.205625\n",
      "iteration 1300 / 2000: loss 2.118189\n",
      "iteration 1400 / 2000: loss 2.210973\n",
      "iteration 1500 / 2000: loss 2.217974\n",
      "iteration 1600 / 2000: loss 2.120867\n",
      "iteration 1700 / 2000: loss 2.128264\n",
      "iteration 1800 / 2000: loss 2.171597\n",
      "iteration 1900 / 2000: loss 2.239067\n",
      "iteration 0 / 2000: loss 1624.447401\n",
      "iteration 100 / 2000: loss 453.050621\n",
      "iteration 200 / 2000: loss 127.651568\n",
      "iteration 300 / 2000: loss 37.118736\n",
      "iteration 400 / 2000: loss 11.904699\n",
      "iteration 500 / 2000: loss 4.863768\n",
      "iteration 600 / 2000: loss 2.885833\n",
      "iteration 700 / 2000: loss 2.434016\n",
      "iteration 800 / 2000: loss 2.211460\n",
      "iteration 900 / 2000: loss 2.176015\n",
      "iteration 1000 / 2000: loss 2.180215\n",
      "iteration 1100 / 2000: loss 2.208885\n",
      "iteration 1200 / 2000: loss 2.110132\n",
      "iteration 1300 / 2000: loss 2.167402\n",
      "iteration 1400 / 2000: loss 2.180747\n",
      "iteration 1500 / 2000: loss 2.195586\n",
      "iteration 1600 / 2000: loss 2.191523\n",
      "iteration 1700 / 2000: loss 2.196069\n",
      "iteration 1800 / 2000: loss 2.183318\n",
      "iteration 1900 / 2000: loss 2.151085\n",
      "iteration 0 / 2000: loss 218.674661\n",
      "iteration 100 / 2000: loss 177.743317\n",
      "iteration 200 / 2000: loss 145.754403\n",
      "iteration 300 / 2000: loss 120.074400\n",
      "iteration 400 / 2000: loss 98.265354\n",
      "iteration 500 / 2000: loss 81.044823\n",
      "iteration 600 / 2000: loss 66.699773\n",
      "iteration 700 / 2000: loss 55.288401\n",
      "iteration 800 / 2000: loss 45.680473\n",
      "iteration 900 / 2000: loss 37.788461\n",
      "iteration 1000 / 2000: loss 31.464290\n",
      "iteration 1100 / 2000: loss 26.127527\n",
      "iteration 1200 / 2000: loss 21.750573\n",
      "iteration 1300 / 2000: loss 18.178907\n",
      "iteration 1400 / 2000: loss 15.310978\n",
      "iteration 1500 / 2000: loss 12.869935\n",
      "iteration 1600 / 2000: loss 11.039039\n",
      "iteration 1700 / 2000: loss 9.277131\n",
      "iteration 1800 / 2000: loss 8.118318\n",
      "iteration 1900 / 2000: loss 6.983023\n",
      "iteration 0 / 2000: loss 249.294594\n",
      "iteration 100 / 2000: loss 196.975407\n",
      "iteration 200 / 2000: loss 157.150608\n",
      "iteration 300 / 2000: loss 125.427766\n",
      "iteration 400 / 2000: loss 100.484589\n",
      "iteration 500 / 2000: loss 80.403055\n",
      "iteration 600 / 2000: loss 64.597798\n",
      "iteration 700 / 2000: loss 51.982066\n",
      "iteration 800 / 2000: loss 41.750845\n",
      "iteration 900 / 2000: loss 33.726583\n",
      "iteration 1000 / 2000: loss 27.474897\n",
      "iteration 1100 / 2000: loss 22.156499\n",
      "iteration 1200 / 2000: loss 18.064532\n",
      "iteration 1300 / 2000: loss 15.012671\n",
      "iteration 1400 / 2000: loss 12.380630\n",
      "iteration 1500 / 2000: loss 10.237782\n",
      "iteration 1600 / 2000: loss 8.518787\n",
      "iteration 1700 / 2000: loss 7.220428\n",
      "iteration 1800 / 2000: loss 6.125445\n",
      "iteration 1900 / 2000: loss 5.227408\n",
      "iteration 0 / 2000: loss 282.482561\n",
      "iteration 100 / 2000: loss 218.039811\n",
      "iteration 200 / 2000: loss 169.322762\n",
      "iteration 300 / 2000: loss 131.612861\n",
      "iteration 400 / 2000: loss 102.393542\n",
      "iteration 500 / 2000: loss 79.803859\n",
      "iteration 600 / 2000: loss 62.468850\n",
      "iteration 700 / 2000: loss 48.895057\n",
      "iteration 800 / 2000: loss 38.376785\n",
      "iteration 900 / 2000: loss 30.170474\n",
      "iteration 1000 / 2000: loss 23.827523\n",
      "iteration 1100 / 2000: loss 18.960403\n",
      "iteration 1200 / 2000: loss 15.144652\n",
      "iteration 1300 / 2000: loss 12.265067\n",
      "iteration 1400 / 2000: loss 10.047921\n",
      "iteration 1500 / 2000: loss 8.171504\n",
      "iteration 1600 / 2000: loss 6.718929\n",
      "iteration 1700 / 2000: loss 5.677508\n",
      "iteration 1800 / 2000: loss 4.814759\n",
      "iteration 1900 / 2000: loss 4.301028\n",
      "iteration 0 / 2000: loss 315.215250\n",
      "iteration 100 / 2000: loss 236.439458\n",
      "iteration 200 / 2000: loss 178.669075\n",
      "iteration 300 / 2000: loss 135.001087\n",
      "iteration 400 / 2000: loss 102.084908\n",
      "iteration 500 / 2000: loss 77.596208\n",
      "iteration 600 / 2000: loss 58.913052\n",
      "iteration 700 / 2000: loss 45.067341\n",
      "iteration 800 / 2000: loss 34.437809\n",
      "iteration 900 / 2000: loss 26.471204\n",
      "iteration 1000 / 2000: loss 20.429806\n",
      "iteration 1100 / 2000: loss 15.959126\n",
      "iteration 1200 / 2000: loss 12.466825\n",
      "iteration 1300 / 2000: loss 10.003890\n",
      "iteration 1400 / 2000: loss 7.971824\n",
      "iteration 1500 / 2000: loss 6.453270\n",
      "iteration 1600 / 2000: loss 5.357852\n",
      "iteration 1700 / 2000: loss 4.582371\n",
      "iteration 1800 / 2000: loss 4.020702\n",
      "iteration 1900 / 2000: loss 3.496648\n",
      "iteration 0 / 2000: loss 345.292154\n",
      "iteration 100 / 2000: loss 252.915533\n",
      "iteration 200 / 2000: loss 185.502527\n",
      "iteration 300 / 2000: loss 136.314723\n",
      "iteration 400 / 2000: loss 100.744653\n",
      "iteration 500 / 2000: loss 74.251290\n",
      "iteration 600 / 2000: loss 54.973898\n",
      "iteration 700 / 2000: loss 40.793242\n",
      "iteration 800 / 2000: loss 30.584609\n",
      "iteration 900 / 2000: loss 22.914373\n",
      "iteration 1000 / 2000: loss 17.274089\n",
      "iteration 1100 / 2000: loss 13.238819\n",
      "iteration 1200 / 2000: loss 10.281207\n",
      "iteration 1300 / 2000: loss 8.066752\n",
      "iteration 1400 / 2000: loss 6.394868\n",
      "iteration 1500 / 2000: loss 5.286296\n",
      "iteration 1600 / 2000: loss 4.382198\n",
      "iteration 1700 / 2000: loss 3.769350\n",
      "iteration 1800 / 2000: loss 3.424265\n",
      "iteration 1900 / 2000: loss 3.018708\n",
      "iteration 0 / 2000: loss 375.640869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: loss 267.104841\n",
      "iteration 200 / 2000: loss 190.883157\n",
      "iteration 300 / 2000: loss 136.495426\n",
      "iteration 400 / 2000: loss 97.702310\n",
      "iteration 500 / 2000: loss 70.406625\n",
      "iteration 600 / 2000: loss 50.674147\n",
      "iteration 700 / 2000: loss 36.851441\n",
      "iteration 800 / 2000: loss 26.914822\n",
      "iteration 900 / 2000: loss 19.682786\n",
      "iteration 1000 / 2000: loss 14.593954\n",
      "iteration 1100 / 2000: loss 11.002157\n",
      "iteration 1200 / 2000: loss 8.335737\n",
      "iteration 1300 / 2000: loss 6.597940\n",
      "iteration 1400 / 2000: loss 5.255831\n",
      "iteration 1500 / 2000: loss 4.432327\n",
      "iteration 1600 / 2000: loss 3.731465\n",
      "iteration 1700 / 2000: loss 3.237194\n",
      "iteration 1800 / 2000: loss 2.899099\n",
      "iteration 1900 / 2000: loss 2.697957\n",
      "iteration 0 / 2000: loss 404.831515\n",
      "iteration 100 / 2000: loss 279.600897\n",
      "iteration 200 / 2000: loss 194.261099\n",
      "iteration 300 / 2000: loss 135.117318\n",
      "iteration 400 / 2000: loss 94.295955\n",
      "iteration 500 / 2000: loss 65.926750\n",
      "iteration 600 / 2000: loss 46.302908\n",
      "iteration 700 / 2000: loss 32.686796\n",
      "iteration 800 / 2000: loss 23.369648\n",
      "iteration 900 / 2000: loss 16.840845\n",
      "iteration 1000 / 2000: loss 12.359893\n",
      "iteration 1100 / 2000: loss 9.205622\n",
      "iteration 1200 / 2000: loss 7.066000\n",
      "iteration 1300 / 2000: loss 5.480771\n",
      "iteration 1400 / 2000: loss 4.427825\n",
      "iteration 1500 / 2000: loss 3.660892\n",
      "iteration 1600 / 2000: loss 3.204747\n",
      "iteration 1700 / 2000: loss 2.904955\n",
      "iteration 1800 / 2000: loss 2.647060\n",
      "iteration 1900 / 2000: loss 2.462325\n",
      "iteration 0 / 2000: loss 1453.284684\n",
      "iteration 100 / 2000: loss 387.931398\n",
      "iteration 200 / 2000: loss 104.846447\n",
      "iteration 300 / 2000: loss 29.553162\n",
      "iteration 400 / 2000: loss 9.499428\n",
      "iteration 500 / 2000: loss 4.120360\n",
      "iteration 600 / 2000: loss 2.688865\n",
      "iteration 700 / 2000: loss 2.336087\n",
      "iteration 800 / 2000: loss 2.179235\n",
      "iteration 900 / 2000: loss 2.205673\n",
      "iteration 1000 / 2000: loss 2.162099\n",
      "iteration 1100 / 2000: loss 2.169576\n",
      "iteration 1200 / 2000: loss 2.155457\n",
      "iteration 1300 / 2000: loss 2.155005\n",
      "iteration 1400 / 2000: loss 2.129668\n",
      "iteration 1500 / 2000: loss 2.191301\n",
      "iteration 1600 / 2000: loss 2.171150\n",
      "iteration 1700 / 2000: loss 2.134016\n",
      "iteration 1800 / 2000: loss 2.199675\n",
      "iteration 1900 / 2000: loss 2.144745\n",
      "iteration 0 / 2000: loss 1490.167492\n",
      "iteration 100 / 2000: loss 386.595993\n",
      "iteration 200 / 2000: loss 101.644265\n",
      "iteration 300 / 2000: loss 27.857632\n",
      "iteration 400 / 2000: loss 8.804456\n",
      "iteration 500 / 2000: loss 3.896488\n",
      "iteration 600 / 2000: loss 2.613266\n",
      "iteration 700 / 2000: loss 2.252218\n",
      "iteration 800 / 2000: loss 2.199995\n",
      "iteration 900 / 2000: loss 2.157406\n",
      "iteration 1000 / 2000: loss 2.223315\n",
      "iteration 1100 / 2000: loss 2.109335\n",
      "iteration 1200 / 2000: loss 2.184421\n",
      "iteration 1300 / 2000: loss 2.195556\n",
      "iteration 1400 / 2000: loss 2.212935\n",
      "iteration 1500 / 2000: loss 2.188496\n",
      "iteration 1600 / 2000: loss 2.201367\n",
      "iteration 1700 / 2000: loss 2.199020\n",
      "iteration 1800 / 2000: loss 2.151502\n",
      "iteration 1900 / 2000: loss 2.150309\n",
      "iteration 0 / 2000: loss 1477.098762\n",
      "iteration 100 / 2000: loss 372.720530\n",
      "iteration 200 / 2000: loss 95.263709\n",
      "iteration 300 / 2000: loss 25.596467\n",
      "iteration 400 / 2000: loss 8.045928\n",
      "iteration 500 / 2000: loss 3.648191\n",
      "iteration 600 / 2000: loss 2.569705\n",
      "iteration 700 / 2000: loss 2.218926\n",
      "iteration 800 / 2000: loss 2.196480\n",
      "iteration 900 / 2000: loss 2.133283\n",
      "iteration 1000 / 2000: loss 2.152904\n",
      "iteration 1100 / 2000: loss 2.194202\n",
      "iteration 1200 / 2000: loss 2.179414\n",
      "iteration 1300 / 2000: loss 2.222866\n",
      "iteration 1400 / 2000: loss 2.162197\n",
      "iteration 1500 / 2000: loss 2.188692\n",
      "iteration 1600 / 2000: loss 2.130824\n",
      "iteration 1700 / 2000: loss 2.187288\n",
      "iteration 1800 / 2000: loss 2.270365\n",
      "iteration 1900 / 2000: loss 2.142194\n",
      "iteration 0 / 2000: loss 1539.828979\n",
      "iteration 100 / 2000: loss 377.936106\n",
      "iteration 200 / 2000: loss 93.979918\n",
      "iteration 300 / 2000: loss 24.628010\n",
      "iteration 400 / 2000: loss 7.687494\n",
      "iteration 500 / 2000: loss 3.478808\n",
      "iteration 600 / 2000: loss 2.552265\n",
      "iteration 700 / 2000: loss 2.282499\n",
      "iteration 800 / 2000: loss 2.194560\n",
      "iteration 900 / 2000: loss 2.242379\n",
      "iteration 1000 / 2000: loss 2.161299\n",
      "iteration 1100 / 2000: loss 2.143296\n",
      "iteration 1200 / 2000: loss 2.146886\n",
      "iteration 1300 / 2000: loss 2.194150\n",
      "iteration 1400 / 2000: loss 2.136447\n",
      "iteration 1500 / 2000: loss 2.159583\n",
      "iteration 1600 / 2000: loss 2.163369\n",
      "iteration 1700 / 2000: loss 2.149931\n",
      "iteration 1800 / 2000: loss 2.155806\n",
      "iteration 1900 / 2000: loss 2.149725\n",
      "iteration 0 / 2000: loss 1576.703775\n",
      "iteration 100 / 2000: loss 376.031107\n",
      "iteration 200 / 2000: loss 90.983920\n",
      "iteration 300 / 2000: loss 23.287243\n",
      "iteration 400 / 2000: loss 7.168935\n",
      "iteration 500 / 2000: loss 3.362532\n",
      "iteration 600 / 2000: loss 2.455520\n",
      "iteration 700 / 2000: loss 2.210617\n",
      "iteration 800 / 2000: loss 2.210380\n",
      "iteration 900 / 2000: loss 2.097438\n",
      "iteration 1000 / 2000: loss 2.194138\n",
      "iteration 1100 / 2000: loss 2.185077\n",
      "iteration 1200 / 2000: loss 2.200069\n",
      "iteration 1300 / 2000: loss 2.173781\n",
      "iteration 1400 / 2000: loss 2.226718\n",
      "iteration 1500 / 2000: loss 2.194325\n",
      "iteration 1600 / 2000: loss 2.117163\n",
      "iteration 1700 / 2000: loss 2.199957\n",
      "iteration 1800 / 2000: loss 2.128196\n",
      "iteration 1900 / 2000: loss 2.162455\n",
      "iteration 0 / 2000: loss 1594.997097\n",
      "iteration 100 / 2000: loss 369.480377\n",
      "iteration 200 / 2000: loss 87.018649\n",
      "iteration 300 / 2000: loss 21.800072\n",
      "iteration 400 / 2000: loss 6.674740\n",
      "iteration 500 / 2000: loss 3.248490\n",
      "iteration 600 / 2000: loss 2.445261\n",
      "iteration 700 / 2000: loss 2.203425\n",
      "iteration 800 / 2000: loss 2.211605\n",
      "iteration 900 / 2000: loss 2.209166\n",
      "iteration 1000 / 2000: loss 2.219182\n",
      "iteration 1100 / 2000: loss 2.152394\n",
      "iteration 1200 / 2000: loss 2.205875\n",
      "iteration 1300 / 2000: loss 2.219751\n",
      "iteration 1400 / 2000: loss 2.159632\n",
      "iteration 1500 / 2000: loss 2.154721\n",
      "iteration 1600 / 2000: loss 2.217651\n",
      "iteration 1700 / 2000: loss 2.192427\n",
      "iteration 1800 / 2000: loss 2.211647\n",
      "iteration 1900 / 2000: loss 2.145433\n",
      "iteration 0 / 2000: loss 1634.189648\n",
      "iteration 100 / 2000: loss 368.311815\n",
      "iteration 200 / 2000: loss 84.439477\n",
      "iteration 300 / 2000: loss 20.683009\n",
      "iteration 400 / 2000: loss 6.336562\n",
      "iteration 500 / 2000: loss 3.056348\n",
      "iteration 600 / 2000: loss 2.380013\n",
      "iteration 700 / 2000: loss 2.205008\n",
      "iteration 800 / 2000: loss 2.128918\n",
      "iteration 900 / 2000: loss 2.196371\n",
      "iteration 1000 / 2000: loss 2.122284\n",
      "iteration 1100 / 2000: loss 2.240236\n",
      "iteration 1200 / 2000: loss 2.166759\n",
      "iteration 1300 / 2000: loss 2.147509\n",
      "iteration 1400 / 2000: loss 2.122196\n",
      "iteration 1500 / 2000: loss 2.152685\n",
      "iteration 1600 / 2000: loss 2.189313\n",
      "iteration 1700 / 2000: loss 2.147780\n",
      "iteration 1800 / 2000: loss 2.118637\n",
      "iteration 1900 / 2000: loss 2.200110\n",
      "iteration 0 / 2000: loss 220.743258\n",
      "iteration 100 / 2000: loss 175.646090\n",
      "iteration 200 / 2000: loss 140.024736\n",
      "iteration 300 / 2000: loss 111.946024\n",
      "iteration 400 / 2000: loss 89.616055\n",
      "iteration 500 / 2000: loss 71.845017\n",
      "iteration 600 / 2000: loss 57.646256\n",
      "iteration 700 / 2000: loss 46.373495\n",
      "iteration 800 / 2000: loss 37.541340\n",
      "iteration 900 / 2000: loss 30.161070\n",
      "iteration 1000 / 2000: loss 24.404321\n",
      "iteration 1100 / 2000: loss 19.877859\n",
      "iteration 1200 / 2000: loss 16.218367\n",
      "iteration 1300 / 2000: loss 13.470253\n",
      "iteration 1400 / 2000: loss 11.083997\n",
      "iteration 1500 / 2000: loss 9.292395\n",
      "iteration 1600 / 2000: loss 7.865689\n",
      "iteration 1700 / 2000: loss 6.597072\n",
      "iteration 1800 / 2000: loss 5.693061\n",
      "iteration 1900 / 2000: loss 4.905160\n",
      "iteration 0 / 2000: loss 254.224045\n",
      "iteration 100 / 2000: loss 195.069161\n",
      "iteration 200 / 2000: loss 150.369420\n",
      "iteration 300 / 2000: loss 116.694176\n",
      "iteration 400 / 2000: loss 90.614973\n",
      "iteration 500 / 2000: loss 70.235059\n",
      "iteration 600 / 2000: loss 54.911728\n",
      "iteration 700 / 2000: loss 42.760637\n",
      "iteration 800 / 2000: loss 33.466616\n",
      "iteration 900 / 2000: loss 26.312245\n",
      "iteration 1000 / 2000: loss 20.732321\n",
      "iteration 1100 / 2000: loss 16.552323\n",
      "iteration 1200 / 2000: loss 13.175977\n",
      "iteration 1300 / 2000: loss 10.672458\n",
      "iteration 1400 / 2000: loss 8.678468\n",
      "iteration 1500 / 2000: loss 7.170657\n",
      "iteration 1600 / 2000: loss 5.881514\n",
      "iteration 1700 / 2000: loss 5.139601\n",
      "iteration 1800 / 2000: loss 4.358626\n",
      "iteration 1900 / 2000: loss 3.841440\n",
      "iteration 0 / 2000: loss 280.502973\n",
      "iteration 100 / 2000: loss 209.315763\n",
      "iteration 200 / 2000: loss 156.609939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: loss 117.643482\n",
      "iteration 400 / 2000: loss 88.402880\n",
      "iteration 500 / 2000: loss 66.496967\n",
      "iteration 600 / 2000: loss 50.199100\n",
      "iteration 700 / 2000: loss 38.182242\n",
      "iteration 800 / 2000: loss 29.094235\n",
      "iteration 900 / 2000: loss 22.217552\n",
      "iteration 1000 / 2000: loss 17.139682\n",
      "iteration 1100 / 2000: loss 13.337412\n",
      "iteration 1200 / 2000: loss 10.404857\n",
      "iteration 1300 / 2000: loss 8.336160\n",
      "iteration 1400 / 2000: loss 6.719057\n",
      "iteration 1500 / 2000: loss 5.523558\n",
      "iteration 1600 / 2000: loss 4.757710\n",
      "iteration 1700 / 2000: loss 4.067460\n",
      "iteration 1800 / 2000: loss 3.450223\n",
      "iteration 1900 / 2000: loss 3.060256\n",
      "iteration 0 / 2000: loss 311.010838\n",
      "iteration 100 / 2000: loss 223.906269\n",
      "iteration 200 / 2000: loss 162.338403\n",
      "iteration 300 / 2000: loss 117.906687\n",
      "iteration 400 / 2000: loss 85.988996\n",
      "iteration 500 / 2000: loss 62.739017\n",
      "iteration 600 / 2000: loss 45.902502\n",
      "iteration 700 / 2000: loss 33.741439\n",
      "iteration 800 / 2000: loss 25.138719\n",
      "iteration 900 / 2000: loss 18.752416\n",
      "iteration 1000 / 2000: loss 14.047575\n",
      "iteration 1100 / 2000: loss 10.844902\n",
      "iteration 1200 / 2000: loss 8.345375\n",
      "iteration 1300 / 2000: loss 6.637247\n",
      "iteration 1400 / 2000: loss 5.311744\n",
      "iteration 1500 / 2000: loss 4.408465\n",
      "iteration 1600 / 2000: loss 3.732912\n",
      "iteration 1700 / 2000: loss 3.332028\n",
      "iteration 1800 / 2000: loss 2.922849\n",
      "iteration 1900 / 2000: loss 2.683202\n",
      "iteration 0 / 2000: loss 338.877808\n",
      "iteration 100 / 2000: loss 235.503544\n",
      "iteration 200 / 2000: loss 165.503080\n",
      "iteration 300 / 2000: loss 116.446443\n",
      "iteration 400 / 2000: loss 82.369901\n",
      "iteration 500 / 2000: loss 58.409206\n",
      "iteration 600 / 2000: loss 41.492298\n",
      "iteration 700 / 2000: loss 29.738270\n",
      "iteration 800 / 2000: loss 21.505595\n",
      "iteration 900 / 2000: loss 15.636226\n",
      "iteration 1000 / 2000: loss 11.597464\n",
      "iteration 1100 / 2000: loss 8.806332\n",
      "iteration 1200 / 2000: loss 6.792321\n",
      "iteration 1300 / 2000: loss 5.302375\n",
      "iteration 1400 / 2000: loss 4.281692\n",
      "iteration 1500 / 2000: loss 3.619994\n",
      "iteration 1600 / 2000: loss 3.189272\n",
      "iteration 1700 / 2000: loss 2.915552\n",
      "iteration 1800 / 2000: loss 2.539823\n",
      "iteration 1900 / 2000: loss 2.368596\n",
      "iteration 0 / 2000: loss 377.754382\n",
      "iteration 100 / 2000: loss 255.847445\n",
      "iteration 200 / 2000: loss 174.013903\n",
      "iteration 300 / 2000: loss 118.767308\n",
      "iteration 400 / 2000: loss 81.284816\n",
      "iteration 500 / 2000: loss 55.946664\n",
      "iteration 600 / 2000: loss 38.477855\n",
      "iteration 700 / 2000: loss 26.905892\n",
      "iteration 800 / 2000: loss 18.834133\n",
      "iteration 900 / 2000: loss 13.552023\n",
      "iteration 1000 / 2000: loss 9.865454\n",
      "iteration 1100 / 2000: loss 7.421786\n",
      "iteration 1200 / 2000: loss 5.624293\n",
      "iteration 1300 / 2000: loss 4.540025\n",
      "iteration 1400 / 2000: loss 3.694819\n",
      "iteration 1500 / 2000: loss 3.115266\n",
      "iteration 1600 / 2000: loss 2.758361\n",
      "iteration 1700 / 2000: loss 2.528040\n",
      "iteration 1800 / 2000: loss 2.350210\n",
      "iteration 1900 / 2000: loss 2.255540\n",
      "iteration 0 / 2000: loss 407.154265\n",
      "iteration 100 / 2000: loss 266.958200\n",
      "iteration 200 / 2000: loss 176.006350\n",
      "iteration 300 / 2000: loss 116.398779\n",
      "iteration 400 / 2000: loss 77.368569\n",
      "iteration 500 / 2000: loss 51.434315\n",
      "iteration 600 / 2000: loss 34.624220\n",
      "iteration 700 / 2000: loss 23.550466\n",
      "iteration 800 / 2000: loss 16.093122\n",
      "iteration 900 / 2000: loss 11.284679\n",
      "iteration 1000 / 2000: loss 8.199264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-62824aad7a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregularization_strengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtmp_smax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_smax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_smax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML/CS231N/assignment1/cs231n/classifiers/linear_classifier.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, learning_rate, reg, num_iters, batch_size, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;31m#########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;31m# evaluate loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0mX_batch\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2457\u001b[0m     \"\"\"\n\u001b[1;32m   2458\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0melements\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 1.2e-7,1.4e-7,1.6e-7]\n",
    "regularization_strengths =[(1+0.1*i)*1e4 for i in range(-3,4)] + [(5+0.1*i)*1e4 for i in range(-3,4)]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        tmp_smax = Softmax()\n",
    "        train_result = tmp_smax.train(X_train,y_train,lr,reg,num_iters = 2000,batch_size = 200,verbose = True)\n",
    "        y_train_pred = tmp_smax.predict(X_train)\n",
    "        train_acc = np.mean(y_train_pred == y_train)\n",
    "        val_y_pred = tmp_smax.predict(X_val)\n",
    "        val_acc = np.mean(val_y_pred)\n",
    "        if best_val<val_acc:\n",
    "            best_val = val_acc\n",
    "            best_softmax = tmp_smax\n",
    "        results[(lr,reg)] = train_acc,val_acc\n",
    "        \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.029000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "print(\"best val is\",best_va)\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "\n",
    "\n",
    "*Your explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADfCAYAAADmzyjKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsvX2wNcldHvb8unvmnHvvflnCxEjoo0AOFb4MxkBcBiNkJZQhLhQVLocKwcLGBTE2BpeNIkVguZAth4KIcrBNgjGUjEkggGNTdlyECAdswJQFBNukhCX0sRJCICxpd+97z5mZ7s4f/Xt+PTPvaveeo9W9+57tp+p9554zc2Z6enq6n9+35JzR0NDQ0HDvw912AxoaGhoanhq0Cb2hoaHhRNAm9IaGhoYTQZvQGxoaGk4EbUJvaGhoOBG0Cb2hoaHhRHDPTugi8mIRefdtt6Ph6Q0ReYeIvPRxvv98EXnLgef6fhF53VPXuoanI+7l53zPTugNDR8Jcs4/k3P+pNtux72ID7dINtw+2oTecBdEJNx2G24Tz/T7b3jqcVNj6mk/oSsbeJWI/KqIfEBEvk9Eto9z3H8nIm8TkUf12P9ytu8VIvIvROTb9RxvF5E/Otv/oIh8r4i8V0TeIyKvExF/U/f4VENEniciPyYivy0ivyMi3yUinygib9LP7xeRfyAiD81+8w4ReaWI/AqAyxOb1D57PX7WKrvHu38R+UwR+UUdUz8E4K5xd6/j0LEiIn8fwPMB/LiIPCYi33S7d/CR44mes4j8FyLyyyLyQRH5WRH59Nm+54jIj2rfvV1Evn6277Ui8iMi8gMi8giAV9zIzeScn9b/ALwDwL8F8DwAzwLwLwG8DsCLAbx7dtwfB/AclEXqTwC4BPBxuu8VAEYAfwaAB/DfAvgNAKL7/w8A/zOACwAfC+AXAHzNbd/7kf3lAfy/AN6g97MF8HkAXgTgPwOwAfC7Afw0gO9c9fMvaz+f3fZ93ML4Wdw/gB7AOwF8I4AOwJfpGHrdbd/T02SsvPS22/8U9cGHfc4Afj+A3wLwudpXf1LvfaPzzJsBfIue4xMA/DqAL9LzvlbP8zI99kbeqVvv0Gt0+DsAfO3s8xcDeNv6hXyc3/0ygC/Vv18B4K2zfecAMoDfA+A/ArCfdziALwfwU7d970f21x8E8NsAwpMc9zIAv7Tq5z912+2/rfGzvn8AfxizRV+/+9kTm9A/krFyKhP6h33OAP4OgG9dHf8WAF+gk/y7VvteBeD79O/XAvjpm76fe0Wsfnj29ztRmPgCIvKVAP4igBfqV/cB+JjZIb/JP3LOd0SExzwLZWV+r34HlBV1fs17Cc8D8M6c8zT/UkQ+FsDfBPD5AO5HuccPrH57r97zk+FJx8/jHPccAO/J+nbOfntK+EjGyqngiZ7zCwD8SRH587N9vf4mAniOiHxwts8D+JnZ5xt/n572OnTF82Z/Px9lRTWIyAsAfA+APwfg2Tnnh1DEbMGT42EUhv4xOeeH9N8DOedPeWqafuN4GMDzH0cH/noUqeTTc84PAPgK3N0/p5p68wnHzwzz+38vgOfKbJXX354Sjh0rpzROnug5Pwzgr83mhYdyzuc55/9V9719te/+nPMXz85z4/10r0zoXyciHy8izwLwagA/tNp/gdJ5vw0AIvJVAD71OifOOb8XwE8A+A4ReUBEnBqFvuCpa/6N4hdQBunfEJELNQD+IRSm9RiAD4rIcwH85dts5A3jycbP4+HnAEwAvl4NpC8H8DkfzUbeAo4dK+9D0RmfAp7oOX8PgK8Vkc+VggsR+RIRuR+l7x5RQ/qZiHgR+VQR+exbug8A986E/oMok+6v67+F03/O+VcBfAfKw3kfgE9DMX5dF1+JIkr9Kopo+SMAPu4jbvUtIOccAfwxFMPWuwC8G8VI/FdRjDwfAvBPAPzYbbXxFvCE4+fxkHMeALwcxf7yAZQ+PKk++wjGyusBvEY9P/7SzbX4qccTPeec879GcaT4Lt33Vj1u3nefAeDtAN4P4O8CePAm27+GLFVHTz+IyDsAfHXO+Sdvuy0NDQ0NT2fcKwy9oaGhoeFJ0Cb0hoaGhhPB017l0tDQ0NBwPTSG3tDQ0HAiuNHAoj/1rf8yA0BMCQCQUgLdW8XpVt1Bs36fU0bOSc9AaUKP1c/zb5MeG2PZUgJxXlOziLNrCF1r9Tf0RHXO2TWwvnbmeWH3UI4Hgi/b7/srX3Ad/3cAwN/671+TS/ucNs8jp6x/sy/0HqwNGSJu1hOA+PKX90G35X5TTIgx272X8yXbx/vtQjm+68LydvkcZm0W5/W7uDxW251Ssmc8TmX7Nd/y6mv3yTd+80szAHSbvlxPxJ5jtmcleq3ym5gSYppWZ+IzXGwgInDaFzYuuFd/Mo4Tcox6TY6z2v/8zL6Iq2N77c8QQj1tWo7J73zdT127TwDg27785SVeX8dbTrm+L3pO7sOsn+LqugTvh8/Ne2/HpKncj+hPOD69c8jaam/jScerdl4IwcafkDLqjybtpymWZ5WyYNRxyH2v+t//4bX75Uu/4rMyAGzONgCALvjZc3bWB/N7cE5m8wwWfVC/137I9f12+j7y83xMJvYb+4s3zu9jRJz0nmN9T4CZgz+PTdHGE/v/R9/4i9fqk8bQGxoaGk4ENxv6b6ySK5MYC+WqZ+sVGZhkW93zao1yZNqk1lJ/HpUZkrE7ZVKFkZG5LRkXV+B5UBxZGrBiOdzGCK/nNnZ0AKY4AgB6v7HTRrIFY4LaX45sUox95JW0kXWNJuvJKRtDE1my3HFURpuBUe+9V4ZOZknGMQ+kC/2S+RSX3BljSdEYT04RhyJ0HQCg6wtDd84Zm6FUYcPEpI9kbJvPura5Snu6w/oP8vjERzAiSnk2os816b1wmwEIyNbYJ+X3Xhm6N2YrxprJTg/FdnsOvRi0Ifbc2T8cw3zG3glEn3NS1kmmyPHA98ghmURmPafnC/b+uMdhtZW9A0AQj2CSj2Ilgc8lLLbDr39zDXQ9JSG9dvBgolTK1mTPnD+yOHum5Ogmfa0YusBZ222fW44ZgTNmzPmC46JKypPNcdQemBaBEhR/6xyC9imf1XXRGHpDQ0PDieBGGTpZBFdFrszlb+5ZrpQiMlPE8edpcYyxVHH2neuUsWCpP/MzHTXZO5kwWZUASGm5qlu6i7xkOSnnmfL9IJUoAGDY7RbthLiq89Rj2GsxUJLwM10pr6kMkbrKcdRtNCZQpRVlR9G4FjplYJPmadqsJCXkjKjMNOSV7s+eq7Zk9rjiEWy02xRpxYdOWyfwnraWuf0FcBxDKRtDD57DeilJmBSTpbI1XnT1Rw4RidKZXoPMepxK3xbpo5yoc7RdUDdP1qsMt/M2PPyBrItwrtP7UEgszB+VhXqjitXmk7XdkniHtCstpeIUo0mB7EvBckynmJAp6ZnuPSyOFZG7mH4w/TWldGWgLpueWFKdD64LSnPBJCJv9+D1uceq2C7XcW5mMil/dD3tAUubROk0fZbr99w2Yt95/oyX5Pmch/Rk6GpHGDiOyNRVAozRfj+5w6SWm53QqdrQz85JFd8yDYH6wAMNb/WYKkAvJ3g7b/BVDSB+cSwfuBOpk/VqUuDLiCw2YURK5mwnxaX6VtUJ4xiVyziULV+KEKph11RNet6ZtsEGKUUz3pPOQfuhHDwMky0QVEUsDHVQdYAOyInnTctBnHM2A6fEve4rh1I81DmsvPT6TIb9/qD+AOpLKmZ8klnfLlVLMleXWV/wSC7mYbEVCKp9cL0I6499BsJy8cjQF1DVK9nFOnGtVDecuDimQt/B2UI4Xqcb7kLW8eBCnXTDXeI/Z+26JXFKZnxcLiimfkKqk8tKs0gVToqpki4uZp1+UvVHytl61QyKq0mfasSiNuKzfNIuuAu9quWocnEis4lbJ/bAceDZqKoSNKPv0kha1Wu4y2HD5ij7vqqhOiUhfqXmSylZn0xqcPZUDdlEriRxijbrHTqnNJVLQ0NDw4ngRhm6uUTRYOPdjNmYjFJgdFzM6FiNcEvXNWNp3tcVbU28ZgYy35GJc1UvHztdwXPOmEYa86oBCgAyRcdI5i9VjFyrhq6BHJeGSiAbEyPrpqpjjNU4zOuzv6g9mfTGx6lsh8lhjLV/AKBTZkA1i6SMjmwrUCSmykAhzqQSMgmvlCqqWNBXim5i5XiEeoGqFj5LXx02kZTZhRUjnqaItGLdjlKaqstq+n9BVftRkuNzqDIzjb3Aesu+dzZ2KMmklfqO0pBz3tizv2twXg8uFDZK1l8uWY2fpR28Q5XyptGOYb/w9/ZsVM2WZyqPyiyXLnYQh5iXjB9CiUFVO87BmQBJ90V9Bjq2fajukTb0j0C3KWPFXvsMOPa5V7bsl0ZSiFSHCm1PMs2iqkMmMmSZ9bfYd0Bl8+XZ0nVZn7veL38zTVN1E2Z1y07fH6pgVE2aQ6zzgTvMqaAx9IaGhoYTwY0y9LXN0EnVjZrxkUyDOi3naoCAuWSRaZCB6XbuUmUMbGmccSKmX/RuufIaa8sZoozE56V7U8o0cpXVP8VUgwiOIF7G7qkvj7nqztOSqQ80xAogqfxuSqqPVUYQtS9iLtt9AqasbEG3IZVtp+u5Q0ZPHXomw6fblhpCBZBM9z0+M7JaBkwkO99ad3oQhLrPwkgzKmPkEyZhNB3vLBis2lgYgKPHMJgItb85tsyuQBfFPGFS4ydtB3FasvoyoClZ0mWvfDR3Ol/tFWZg/DCukk+GoP2B2Sin1OX9cpzyowgwUEdLCYOS7soVNKdsEgZbOKqkSilRnDcnBDOcmo5aJarOW3/6ToPDaMdYSVEpJ0Qdw0eo0G3cw1VXaHNuoIRH/TonGSf13acAbu1ZStsuuPq4zPBCmwppvavP2cZPOTaYLWXmyGCGV/00DXo+2vZqIN2hteobQ29oaGg4EdwsQ5/pufgHdVZkyVzFfEe9XDD9sLkyql6u80tdWRZBXIXmz10RASVV5m5VQ6gBIGmQT0qptifQwV8ZwHQ3j1h74RwCsnsyIQ9fGRRdu/TYiYEh8Eiq0N6NupKznbqij8o0RnRI3tnvyj2U3241iMjnBK8uVEHYjrLt9GF1Lhv7ZP9tmG5A3ekidYA5WdtzPoIzUOKyFAOwQWOeY8oOjUEizdjMUsIhnWf7+663cTYpO+274irp7HtXmSt5j5CFVf2q7eJ4obvieiuuRmkdqUOnbaGKHBGhW3osWdCTkcfKqM1bh+/c6n0cp1iD0PSMg74T5vqKyVjoJmyX16KuuQuQvrPrA0BaheHzR847k4LjdHgQWmawjp4/Ids4IGumTp2pClJKpus28m0eUkv7nA9ulqpEbQQmBVUxiF508+A1oEr0Oc/sUnelDlh5+LnK0Ovbfz00ht7Q0NBwIrhhHTp1nJWh0GPCghIygwJUBxWqxTytGDr12F4ZeszJgnAt3mYWGMOPnlZwErhpaeUXJDhGCMy+K79ffp5fwh3oMwpUXbCFbMOZV8ZaH0fCOSaHK2Uze13AyW4GZTsTGYfzpg+fTBoq/Xe+Kcf0TuB030Y7e+tl8ZtJMjpGpui1Iv33t+ppYD7ZfsYYD9eMkhFT8iqMhTvJeMjIyBxnQTFYMj4+FoavyyxY2/y46RZdlfLGyCmBoFOG1VGHXtmV6WndkllRcvLOQSg9xjp2DkH11mHovoCcjInJ6EFmsXfAXYEwlH6zeSkxtiLZGKueW/oMLFlUthNF3V6prQGxtK+Xc2PMeZWigl5dxpZRg5BiPnys9BqEZj5iOcHTN13190w8FyyeI9XUGeaNo+NhlQYkI9MZDg6cN5bMOoRQvZw43aBKA/VYagK45ftIfb32Q4xImXaPw3DDKhftRGamS7FGadpkv4zedEiWSZDCBz8zwEJojJlNs2vjqKVeSVXNE6qr/+LaTmoQQM2wR1F0aZyDyCyI5PBcFBYswiyJoUeygANtnfbXoB1wNSXsabzM5YXbTaUNO32kOz12zAkJNKByMSpbhrdsgoPXhXS0iVxVLdoTG6SaZS8xSEtdE1Xtc67if+cx66/h0C5Bpy8in2+eRwvbhL6clMRn02h4c0XUO5yWqpMUMwadhDix7McSAGUOfsGhRpquxllPQ5zYJEmDYIrlfum2RhfZop3RwB4cFymaeM+0w0q2Z2sTueX9qaSJBkmqROitSO1hpJGz75B0RsoWLKbvpapyYoxVBcXx7pdqlQhg1IfR0ZUVNNbXPDil3XVcJxz+/oQNF/0apMPIVSN8jCLV37gczFg5mVqNBkno/ev7EEdTY5jnJt8DqmVyaT1QF+t1tLcPvgZ4cd/K1XRuKK7tuG5P6L0ddnhDQ0NDw9MVN5ttUWHakJxnfkPKLJidT0XIHKWGuevKSzGeTMNLEbvgvLFZiuKWQdE0KKlmOSO7s8gGsodsq7kRDFN36PnNgOPgVsE4h8DC8X1VH+3NJY8qJhX1qNKICZ26sO326tLIAIlwpvegTHvKiHTFspzwZOHK+KZMbYL1RRwK0zzrGDDhzKWxV+PcxqvhlHES2lnBZ2Q1psmhFAOzEG1jv6GyTLpy0gNPR7C4PGPi9XcAEDW9wqjPLDtnxu1xKsyc48YCx/pgvn/RVAyUzmi88vbs2daqHqM6Q3dLVVWkI1QLABDUcJss73uVmozd8rrmtgtMJhmU7/b6ru3UEG4GPOeBwPB97qIqlGoBX1Vi5sCgLFklqyRi90ipMBg91neYAXApVsnxCJUlx3SwPP6+usxazhkaimfvKacUy6S5ZOp872NOMxu2PlvNXtmp4VdSNsnING6mgpnL+toetouulhQ+KXn13txO84ECbmPoDQ0NDSeCW3FbrJnOkhkG0rQMiAhqjAGqCyFXMgYicEljmG6Ew2SKrqVr4ryCCg2xe2WhQZfjzkLNs2V4osE0DqpjHVcGVPFA4rkPZ6PmaEcJYpKZMa4G/pSd6m7YbzCAIc+lPVtlR9EXht7pOdyQsNf+6SwwhaxL3fmQEGjEy6VvLTxZn4/zgs4t3RW3errzQH07+2YwJu2OSIdQM23WJFT5rhznilnOeAaxkBYnYXUcPcbygjs4HQO7q93idNSPjzEicjzojSZjflWqcgwwywyjJ82q7SoNz7O2Hue26MxtUZ9NjmYPqkxQpQkzxiXsRxrQVZrTZzKsGLE4V+0EK7c+y24Jb26AZoQ2JwXN6e+86dVFj6VO2hIfguH42fK0uyOkOW+BYTOpbpXVlS7HlrHTiz0Lb6y9gAFUTKTWi6uGfWoGmLhv0BQYIczy3i8ONako5VzTJ6xcWjneqZPPcDUB24HZFhtDb2hoaDgR3ChDpzO/uV0hwyuDoX6Tq+k0kilkiFf2qezRLOVkz+rNECXUsGTzUKK+kzr6jJwG3Ve2FpZM5pqinXvSfOXcUsVveZPFI1uagiMYBq8t9GAQc92klHKfssBNr7aCsMVVpNSi/RdUv6o69FEf7fmZYEhkvOqFwuRX1JePO3hlmF77+EJzN5/p+c9dQqdpcy+UkZ/ptpfy25C1r6bRJC93hL7YwtmNCQZEMpVadqbcr0oWMWfLuc0rTppCeKJNQv/Y3blE3Je2mmsjGeVedareW17YYF4gTKFQzicyT51cqysBgHdkgPTaivZc3ZHJqGpcEllbru6K1ElHeqOo8tUBI4OEtK0jmSBTvZodoFbyoWumJa+2qjrZdOWUvtiuTP07XPWcod3A7kHZOHX/4kynfFfdg2vAUgyYfS1WvbpVGFoGS01pgjdpoKY9AIBJpXYmypoVmkLSfcxRnTRxlisXKX/b+6zXpv4+w+wtmYp2snq9B9oKUwacsI9bYFFDQ0PDMxI3q0M3nVFlBE6ZphWUYL3LQdmeRNPdMh5jSktWRZ/XMTmrdkNYHU5lMnGcbBVmKHhWLwimh5UpVT3sVNpBptX3rOxT/XUD0/EeoRq1ai8zhm7BF7qCb/sSYu03D5T77u/DperFt1sN8VeGmFSPOTIgpNtiog8wg5jUo4b9kKfK0Bk8tNFuu9DtViJ8LH2xVRa/UWbu4lU5iA5K6cqyfrojOEOtmqPD03tjpUx5QD/5aN4EDnvaPSJtK+XzwO2dImFcPXKJ3aOPlmuxChMZKfXUfY9wttU/1a+ZkhhrcmJbU/QyEIUsS/uRQS0OwTyJnBz32g2qCzf2J8n0ufRDZ6pk1pQVcebnPfL9YTCSBs85Zdw+hEXxDL2hcp5UmWyytMTVx76cgL/tzIayn2o6iPIH7UK1gISluDhGh87UBxZOn8yzjZJHnvjcqAWIiHzHzHaldrWrMpajFmbJMUHSkm1bnVW1rbhxQtRYjBw0dYYGN1nRCoHZPsxji54sqwIaQVyV2P1h709j6A0NDQ0nglvxcqkeJx5ZIwl7XVsGJntXHaAbk6kMuVpZLVBbjrjCRaRUVliqbgPrUjIqLk4Q6sCsDNUyIU+OEVlpDf2pya7M82SWMpfh3vHgQN2aXjRrwQrJgqBLN3XovStMcRNK1fd+c4FO+2Kjq/5EO4N6spChu36LLFUXDcC8CsyXPg5wrCXKMOdIxq4pASTizF+UdrEE3XhHf64MWCUdh8rQjsmJWsPOSWndLFEZI0PJfBk5OpkdZlDvg3Ff2nN1WcbE/rHHyve7AVc7RpGqvzz1rLQvdB287rtPGbqlGHb6235Epg6XVd5Z45P1LC2q09kLcKDjgoGJnsyDRXLxHUdNvBYZOWoFJZLFpTJxW/L0ttFjeQ+b3tjiyPenK2Ovc/Xdi8b+Kf2qB8xGvYwkYNzXcmqlzUzDrHYJRnZmsZD/I0xQ9u7SplUidlVCY7CCxbbUMHyy7aieT5OOFW7jVRnjcT+YaGseMWyo6tlD31vIP0tnBkpObGfwVtPVPNNn5R3LeTk+vCUWy4+TDPCJcKMTOjUunIyKWMGoFBU3ppWoJhEuMwBCJyZO7GZwKrt77zBF/o5BAAxwZxCRmCjWWX5ssX0AMKZkA8Wy51mNQZ20mOER2Yxv0Vwtr4/ESYJJWZK37HMsPLxFeVHOfFGnBNebAcqr+6LflMk+2sRejpV+Y21lPu1o96KveooQBgIxL4cG3LjE73eWOMayz400lOnLqc9uHDJipNh7+BCbD2ygBALVGGqu4lxQ6fLla75vfY2u1CXx8s4lAGCvhu1xN1hueXNP5TjRhdxtIgIH1mXpvzPH56Iv9jhBIl05y6H2ImYa7BmMVMPMcUR+m9JUqif1NG6momTOdhYJ57HI5m6JwPGz0WNXseyhOhUQngW7qf5K0dwLORnSRVNomM81p0lUQsDcJBYcyMUFGYFj9fpdYSCvsgDC2aqQlGAw7zvTiaRYnR7G2ZgAgEkX+qiqF4yTpQjJlueF5Gv2PC0SaOWaaiohsUAnKxzOnFLdsuZtcVXk4t2Mog0NDQ3PSNwoQ5/U0ElJLbg0y7aoIhkNTxaenqqb0LTMQMaQ4T3P4RxGGiH0qB0NDrpK912P820RI526AXLFZDXucRgsgVhveiIszmM5sTFLjGQh2dcHmQ+zGiI6y3DHgCWKXZlycJetXZ0l9aIRRdVZ23Jv3fmFZZ1jJR+yJdYyBJKxmfEOjcd6jD6ztLuDSVk7DcyySoOw3zFZFwDNUNhtDh9iTHnAgZJRK1tR1VVVGXQlzFW8tapN5TPVEKzctJv2lg6BEpgpy2aMjwyWTD+qxHShapk+3Z1mgNIfM/uR5YecLFfFkZH/xvAtaCVlU/NxHx0GJo5JV1VtFJHFkqgtA1wYyj5vZNcvg9Fcyhbs4juqJbVdvE7MHLqzZFz6DCgUGGt2NWnfEZJLYJZLpt8oPoSlPcw8qn1BdQqmytAtUSD7b1iqXnysqQk49rzVERA7n0mKbJjllSdjt5RmlkmWybp6no/pCyD2DiSmZ7gmGkNvaGhoOBHcKENnkiSjTh0spWhktaBV1ZKUiyETmIURU++s+t79VdGRjilioC5UjMLpx7LdbLZ2jaFTgyyZxcxwSkOK6fpWtU5plMux6tYobRyCzaYEAg2PqbtUFOyVAdN1MwlDz9VtUDpstnx0ek1NTeB7ZVtqVPaDQxzKua16Cmb3CTUSaV1DjOVYoduo6p/HO5fIejyZurF6Pfbq6pFyXlxhU+yn6MPSjfQ6SJYsjYEX3V3JuWqyNXXbjFOtksTkUTZe1L2VrMd5OGWesrKV+J4pYcX033QFDIwHmdWsrC5yy2AoJixj4JybJtP3MnHXoSBTNOk15po4zNxol4EtySV0puDX/lF/XTJCMvOu70ydzsAwGi/Z+DzLJU5X40mN0AyM8U5m9UVpJKYUu5SIBFLzwx/B0M3Vz4KakkkXJlyz32fmF0veRwlKnyPTPdByncapzkl8bkwzYDp0sbD9mt1kydB9cNgwmRfz0VOa61Zuljmby22cDptTGkNvaGhoOBHcKEM3ffmsOk+m1duqeZRjJ2WMQ4yW9tKCj/TYKw3fvrMr7nPDNFRXJTINcwkqq+LVbsSgngwb1S2fqyWfn1OqDH00fZe6ydENkrUMY7LVOB7B0KH68lGDc3b7wRSNw75894iy9/P7Sp9c3Ddis1V9NnW3ag8IGw1CUqkFzmMfl7aHbCH/Gu7vvLHvpIUesjL+Sbe7xx7DpPt2ZO3a/6M+q5TVMyZE3K8eNedHeP4w6MPCxCG1SpXpW8uxJE/eeYhQgqA3TtnHwgt8Zq7vTf/vrRCHnr9n8IeYu5/ovdBLiEEfnffoyLJYT5MSIYNPZrlRhQz7wDqRBJOM0csri5tVKtLrB15fJQZXU8v2Z+oiSwchPZbBeF3f1VD/VapgehyllDDSu0XZo7NbZJh7tXn0Ws0qq02G7ozkkjmKEfN4oIueXkxvpmyCdzY2qNFmAKFY8YkMnzRYzNIBqDdXz2Rr5TdDBhILpaTlXECpxXlnz7/Tghtk3+iq3v2Mlb10K/o8GIRkaXVztccd6uLaGHpDQ0PDieBmGbquOr36wTqRWcgu9Z9MG1mYZogRQ1pb8lV3rhZgfh+nVAOTrE6o6gLJsjpg8qpf1lV4x9QBAz1tPDoyHGXkVs5qZZEvSevp435M1iWG5WsCpTHj8tHCgO9cMkGQJum60pSv+xFdV0LXmZSJHjss40n3AAAgAElEQVSbCw3+0SAPEalpCsxTpFw5av/lmGaUV/Xiyr7pBTBcXeHOnSIJPfLIh8q1Vx5F9APfbgWb+5WZH+Obb/mgmPUpW1CHtyryGsxF1loapO1ipXqG9ZdhvtFQ7SlnK1PH9MCir0JQOpe9N1baq1dUt2GNSurJPTYqIfVkaKpf7RnrwORP2cMSislxJejMO8aq0Cf7ztz0mVaA+uPOYaseT7z/ZFIn7Up3M3Q/k6ILVMcck5X2i3ofnWM6WR1PE0zf7M3jSAN2Ruq3GQg1Ky5ySGco6OWyoS93cDYuGYgoTNAXZ2PG6siV3+0nvke0vel4y8DOjE5qb2HwmVTpJVgdU50nKJnQk6gP5s0SjKnrO6rjaeL7lKrEFw4M/b/RCZ0RmnTbcSLWr73Vj9TMY8zVMU4Y1cl/pwZPCyBkvolIw1iyLGkUu5O55lF2GbE3FyMGFtX80kCpabnZFmMlMxxuWZWFOdh18JaXk6Lc4aK014WGC9l+GnCpwQ2PabQaDZ806u2maMYqGmShwUMd2615SMQ7K9jrAif58hMLpskZnoZdVbHs1VVvMsPXHo9elkXkg48U4ydzzTAPirMcFxt4zT+z2Z4f3Cc2SXGRzzKLzGUgEY1QDBAazfi15YuSy+I26LPrmQfcO7v3vPIKs3zgm41lV9ye6YR4dq7bcm/bvse5qrjOdNv17AtV5egCGXOueV6ODCwa9jr+zdUyVpdD5mPpGA1afhM6b4b3zgyHdEnUMUM3zC6A5b05RmiMNtIkEUFdUlOnpEbfsT10zOTRTmDVneh3UMsc6G+TjeHpiOLZZ7rI9hvmRKpGcHoC0wnDjJEx1OyPJGY6kVs9BS4G2VV3Zh0zdJDg+IeIRYjSJZRqraBjqDvbWM6nGlipbRBGourpHGoB9gN1KE3l0tDQ0HAiuNmaohoO7ZiJru/MsECx3TLtWTY9oE80VKixUEXqGsyhTKPrjZFLYE4LMqZgW8uQp82iGmW7oZF0izMVs3sLytFjdWuxPzFhTxF/OiwIYN525v7YTSMGpRasBZmoarosuUh2+8FYDVURHcsHXRUWvbnc6gW8GZHTzEAHAMKc5U7MuphY3WakobMGXOw0A93l1R1tu6oXNJ8MGcuF25hbYH8EQ6cahaJ4n3JVBZD50S9QCVXsOmP0vUoro7Lmy1BUWMzFcoWE3WPMAbI8Lxlpd3aOs/so7aiUdlHuhSqci/NtzaZoVZYo9UHPX/+iMTodpZoDxokqDmXUfV8Tl7KeK1MtUHrqA3rNwFnzxev7x1QSfI/gsaUBldkCaRxUdhpdzeVCJ4WRkjdVL5KtXWT8pqbge0O3PBlrEM0Rkgvdnv0sH5MZphmYZnlWVLXTB/CN5j4GF1JSzfRRzQO6qYwDKspGvj/UznTB0iuwQlFYBSE55yyHFV0krWYDpc5Qg69Gqwl7WJ80ht7Q0NBwIrglhk79kLMlxVhCRzc+VmiXmvOaxrL90mn/XKv09F2H6UwDdRi6bjmMmZ2v6lh7/Y66xKAuR65zVaFFpqHt48qbPY2vY9VJHxH6T5033Q6jiKUBmFSSGemuqW6DwQ81nzNTAEzLBD9Bg6YSauizZakkG+EtlsxR5Rh1HSMbZ//llIwxm4TEGqLKPLbMJR4EXc/nydDx62PUZ9fTsG3/VSblLOGU6iw3gGd1F73knnVI9R565kvKQMfsjWozYFZOMvTN+Tm2ytDpikbWutmyglQwYxylE1cNAGVrg7YGzU3j4ZJcuWcGADFPu4MwhR8DmTzHcGWG1HmbFMJ+cDTu6vj3Hr32w4aJtixEn8miEiIoQS7dYT3bF1BD3yMloQL2M1MUFBdOGhePSORG9z6Q+mcTp71J/2ozoITS1fc5haWLLN+JOFYXx46ZGSmt0i2WTHuzgah0zzmE7po2rkRqNSsm4WKaDs5vFi03mRtxjofZ5RpDb2hoaDgR3KzbIgNQGGAky6AGoOq0JmWIcN6q1Xf9Ki0AdcxkQ1sxPRzdF2mZNlbqnK3UwazVZLW9bb3VW1y6I7EauOUwFpn9fWCHAOjPiieGVTgRwZirLhKYMXXdXg07SxjFi/Ke7F603fDeUiTQlZN9TjbmJRkTGDWM++pO8Swim0yAmeX9ipWSEjGAw3mHwERPRyT/Ng8lJnLKMKbLsxkT4fmDGKtknVC6qYnq0nttZ5/F8u/vdwzjX0o4Z2fb6u6nXi0MOqJXT98FS6XLRFfUyadV9aQ0RWsXvzsU1cUXtrVq86rL7+g2p9uEhKzP1NGrZUMXXGXjqkvf9B36vrJ1oOrHOwuUyhhYk1elOabTFatG5Gf1B1hbVu+BT87SZdRkbPmIikUM/2IFpFKfgO1gUCEpu258sBTVk+r9t3rISA8iq5PbWY0Ac/dk5Sime9j2ZsPaXtDbSfvYYpByTSTG92jlUUQvKMnJ6jBIbgy9oaGh4RmJm/VDZypJ6rWlMgF6BxBeGVMfxKzVXO07v7Re03c0pmgshimhyNir/lDMl9xYhye7UWmh62rleVvclyycaUslRwgT/RxRsajfaiCQ6tKTK0nGgJpSd6eJtnifY4y40gRZluqA1nnmD6I+W7wlPqN3wtl5tTmUdmcLathrov9BE/9bcQDvjZF0miyM9oRJC4gGpsoN3iz3xzD0NK6q3YwR2Gj/6/OMrIeqQ9jl6jVgBWXICrUv9vob8R6evuV6TxZWz/HTB/S0DZCRM5Ws0s3e+5p+lSmOTQ/K4h+auGqazAMrHRGvAMwYnH52qP75YVXEhCzQpdpnFneRlgzR7FcSTN8+l2iBmqZ4mkZ0Wr92NDuBsslIv/TK1i3QSZgWgA7o0PMH89k/xgZFFs5+dt5VjyPuy0zRXa4TumB9wEmEthPL00c27mD+40x8x3gAvu2bs4Dz+9QTSsdVsMIWPCoaW7dxag5QOn8w1UVKNTXFgV4uN5ttkcYFZu0b9hjpwD8uHfMtZ7Cv+ZKRN4vfP96Ezmx3zP9iVUZA16hok1RvqhdG+em1nVjUoXU6BysHpE6ySCMkMeJyOLhPOp3QGbSyvThH7j4IANhrYNFu1IANRrqlbJVgohmmlsZMN/JFmmXn0P4aGPp2J9k9sk/2NIZOXDzri8mYkMDMeTQGm+WYQSQ1myGNvYfAgsGorhhqSUAWVKOajWq3GLNFkZqd0MT8xemQ4arxeF0weRbx2dukqPer6ienuqU01hefFYLucjOzBWjCyFzbB+a4tlNxnKsKLfp8V9QiJ2JOqCI16pP9wmLJjFDkROwCqvF5VmJRf6X/V+O4zpMWickgoiBizMcqN60SKiZTh/iqSjqoNwiqsZhXxhurYTQqXTp5ndB5UxPZxM0FqFu6FMa+qsr6kfmgOJeUmzq/6HHffUtVS8/gNpanjMMsM2c5d61fv1z8U4xGDpvKpaGhoeEZihtl6Mybvd+pOmXTVwMbHfmViVl+aedq8eaOrkbqjqQrJJmj8zU3jKUBITOn0W8fLZd5Z9dYhTunGgTCJY9islgNShqGBvt7Gg9n6GRYvapcthfnZlBkHggGSYnFS6e7GDqZpVPxcFA2GFM2NsT84DR0MfxdnNRiuyz4S70Frx0EQfNqU/RklZhOVRJnF/Uetipx0MXuEKSJEpzeQwgYrpTF0cjG7H+WtS9bdRwz0jF4Zc8cI1Q9eNTIF8r+vKcaoMPsm2K5wPX3icx9ViUpVQYLzFi0qrvG/WC5TtIRAWhAHYOTprroklgOoGnkc1u6MYbgLE3Emr07G9zVkJv1PHzuaeV26LLMih1TLaPqCtAVMdV87KpysapeK6/OmCaMrE17RFVRum0yqCnnDDDQzQpR85bUddnD1Bys+FWT1SsbZwqFs2j53vejvgv98l7Ozzfozxlcp04KGzXAqwG6PPLlnJJXNXCpciq5iPg+tnzoDQ0NDc9I3ChDzxakonq1OBprNzMm2biuXuJ9dSljmC/d5Vg121z26vpE1zdWd+fan32o7oVkCzQ2Ud+VJmP/dM2iITWv2OPu6gp75gUfmEzr+iBr3qqh8vziAueaMfFSXROZUZFud+MwGaPcae1DGn9Zw3JPw1t21nYx+6R+JgvLyZJSbdSotlXXRqZD6LpgRlSGkPNzr8fc/+D9un0A5/eXv5mp8BDUoAplueMekQyaboG0XtVkDDVhJFkmE0MxHQS7JOZqPGZGRavnyDEa7XeWHkDvmxJEjDBpjwOMefxjZDWnst1f7RAZPn9kYFG2gWuZsyypW7JEZtxWox+rBLGiEO2Ulv1Rx/g+jibFTIF9qP3uaz9Tvy5pzfiZOTTXpHFm/CxjJVreeN2d6zWkDtBro3L6aN+YGSMv3/0aADhau+iObIyYhtMtA8+qPYBJuVgdiufdnnn09G0UZh7dL84Hl2z+o2Q0TktpTiyf/FQdRw7Mnd8YekNDQ8OJ4IZD/5e6onHYm9M/PWAsn7C5oyVYWhxzPWRYc/na3JRyMn27pehduYgFP1vBFWSwllI1J9Mpk+FXdkXdefl+2F1h5HcHhukClVn3yogvLs7x4IMPAgD2I2up3tGj1U0wJjg9Hpdl3zgxHaxWIZqFXpvrpvY1K7mz3Z0L2GgQzbkxc23PuaaM3WzMFbJnSlAydWXoDz30EADg/ocewEYlDkogh8A8RSjRxcpY2O9i+dDp9QJzpaAOnS5ztcI73SFz1Seb3SQtPscUMbLye0fXWupblc05V90W6cFFhs5xY66XkyW4ytPh46Rcn3pasxrBe1ZT4jtBRj0L+rLAHfYZ2fPSda+0Vxm63geD+sxTKEbsdzr26b5ndiV6gKVZPQJ6I8nicy00JNbmYwLzWLHJsXKVOPOo8ZFMmteeSR1ma6t1dYHqDWdd4lL5h+qV0vW0ael1ugznycyX3lcjbRApWTvoJcRkayO9nqgFSJN5B+UDXaEbQ29oaGg4EdysH7rqRId9ZRFkwl3SQg/0wIh0zK/LtiXYsn0aik1/WpFZYEfZMFTfUl3mZLpErthptoqWz/Eun3myNTJ1pjHYX+0wqA79mJqilthIdZTnZ2d48MEHAJTKOgCwVZ06FY9xSrjU0Px+U9Ll7lR/z5QATMUZZ1VtTPfNNAMM1Oo8zpWJr9MGX1zU7yk9WUCSduq2XzL0hx54AOd6HneE50K2JFaDtttbZSGSW9pYWB09pWzMnNdkRZ1ppN2DgT1iHhA1lmHpcZCRIWRdTMWQVqzJV50v66qOrMmal2JgSqnaMo5gosDdwUPBd2YDIKz2rQV0ZTOeMC2ExXzR+8XS1yaT9CyTbWZQn/ZpjNhrARbrT31/rABEjHcV8yBztWr2DB7L9sZaUZtDkGZVfgBAojdNgFUsMj93igViwWfr4Wkt0O+7jaseVVf6vuhOeslBkjF98w5zDAKziaj2iXn8qBTHLfskRWsJvWaui8bQGxoaGk4EN+uHrlGIluBfZpGXFvGnOmBj3W5WSV2ZxqQMkSkApqpjr6HFXGvpNVN9bS11Z0yzI2pqgZSi/V3ZHX3N94vPwzAYa89HhHRT/0/WfHHfBR4iM1W9NlMBk+1M04TLO4WJ3/fgfQCqPu6Oluvb7/Z2LHvCEjl5sjiGjXuc0Q+eqYWVhZOxd12HTj1Bak3N8vszTV51/0Vpy4MPPIBzTWMcjtChk+n5vbbXedxhTS9z4q3FAMrnmhJCLMmRnm9YMkrAIbH4Qah9AMASmUFq8iUrkGD5ho3i2uAZGeKvUijHGyXOGKM9a8hxPMrSrdK/GlKlTUoaVCBLfdb0hJGVNxiHKyXBKUZEsyUsyzLSA2yaJgwDE98tUxmMtBfM7DZrTzIew+1+GDGOk/3u4D6hcwm7VvKs2AiPIlvmPCLItIPoEWmVkqB6pzh7ljHz2ZJZ6yGo0hufR/W4o/SfbU6h9GbSS1r2Y0zRxo07UJy7lSLReaw5KaorEQOKmPlwGdIMzFQufjmh10osFeZaZbnU6SI02cDJlh9cB+tIkTrXQTZVMbIcMy62Kcb68I8QpTmpMkve2cUFfpe2+exCVVTmJldfqvtUBfQQJyu+VNquQb8fx9H6xQw/s7w2AND3G2z66p4I1MLBG7oqhmABRJzQ2e885owG1e1ZrdxzhH6Bfc8qUynt4NwstBuwqlPVwAeMVu2Klk1mkBznHyHi7ZkzH00wFZW+XDnb2sHgN7EJtRrOqrpgtrIAtprwRUdOSDbWD5+4SpuWKqA8VVdGtp/gvfa9MzUA3zubbJnSwMhUtHYzh0vStBZs8jRNlvfH1F2r7Jg5iz0Xm6SoRlsZBKcp2nfxCGMxF6k6SSbwGfAd4DxRXS/rfTKtAydbjltveWpiJXWRfbG8B3F+lstdx5VfqnlSTHXiBk9NI/WyH2OM1cX0wP5oKpeGhoaGE8HNBhahrnpAUa/sdwzKUTGb4fgzQydhKpdVZRNLDYBZVRC3ZKFpJubQ+AATi8sKaUmHUp6FbtPwE1e/0dV1Gquq5QijjjEDbe/Z2dbufUv2TQZjKpdo7GOgBGEJypZhyTHnWfg2xcFlpRSBGKMwCSkw2yXzOkutibja0jgUTJXjzcB8TI5rdierxqQ4GENngArPywAyB8FA9zxKMpSmmA9c5u1cqgSqH12uX1OzwvtjrnBlw9k5C1dn2gG6U3pfxXteKNlzPI6hV0bITJMCrwy6z6y7q21Uz8Y8TtbGEBj4Q6a/TKGBnK29NI7WbhL7TEbJ8RhTXpwnp6qC4thbuJdixqgzkHKVeA5FtFQFqh7LYtLJZGkB9JrqfigO8GE5P0RTvy7VsMjJXFApOVbXS46VCc4vjaKykkxzqmoozlHWf6wZPDKYqEo7vPZ10Rh6Q0NDw4lA1u5VDQ0NDQ33JhpDb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRtAm9oaGh4UTQJvSGhoaGE0Gb0BsaGhpOBG1Cb2hoaDgRnMyELiLfLyKvu+123BZE5JNE5JdE5FER+frbbs9tQETeISIvve123IsQkdeKyA88wf5/JyIvvsEm3dMQkSwiL7rp64abvmDDRw3fBOCf55w/87Yb0nB6yDl/ym234amGiLwDwFfnnH/yttvyVOFkGHoDXgDg3z3eDhHxN9yWexYi0khOwz07Du7ZCV1EPlNEflFVDD8EYDvb92dE5K0i8h9E5B+LyHNm+/5zEXmLiHxIRP62iPw/IvLVt3ITTxFE5E0AvhDAd4nIYyLygyLyd0Tkn4rIJYAvFJEHReSNIvLbIvJOEXmNiDj9vReR7xCR94vI20Xkz6nIeC8O6s8QkV/R5/tDIrIFnnRMZBH5OhH59wD+vRS8QUR+S8/zKyLyqXrsRkS+XUTeJSLvE5HvFpGzW7rXoyAirxSR9+i78xYR+SO6q9cx8qiqWP7A7DemzlL1zI9o/z6q7+Hvu5WbORIi8vcBPB/Aj+s78006Dv60iLwLwJtE5MUi8u7V7+b94EXk1SLyNu2HN4vI8x7nWp8nIg+LyBd+1G8s53zP/QPQA3gngG8E0AH4MgAjgNcBeAmA9wP4/QA2AP4nAD+tv/sYAI8AeDmKuukv6O+++rbv6Snok3/O+wDw/QA+BOAPoSzaWwBvBPCPANwP4IUAfg3An9bjvxbArwL4eAC/C8BPAsgAwm3f14F98A4AvwDgOQCeBeD/03v7sGNCf5cB/F/6mzMAXwTgzQAeAiAA/hMAH6fHfieAf6zH3g/gxwG8/rbv/YA++iQADwN4jn5+IYBPBPBaADsAXwzAA3g9gJ9f9e1L9e/X6nvzZfr+/SUAbwfQ3fb9HTFeeE8v1HHwRgAXOg5eDODdT/Cbvwzg32ifCoDfB+DZszH1Ih1LDwP4nBu5p9vu1CMfxB8G8BsAZPbdz6JM6N8L4Ntm39+ng++FAL4SwM/N9ol29ilO6G+c7fMA9gA+efbd16Do3AHgTQC+Zrbvpbh3J/SvmH3+NgDf/URjQj9nAC+Z7X8JyoL3nwJwq/FyCeATZ9/9QQBvv+17P6CPXgTgt/QZd7PvXwvgJ2efPxnA1apv5xP6fLJ3AN4L4PNv+/6OGC/rCf0TZvufbEJ/C4Av/TDnzgBehUI8P+2m7uleVbk8B8B7svac4p2zffwbOefHAPwOgOfqvodn+zKAhUh1Qnh49vfHoEo1xDtR+gRY9cvq73sNvzn7+w7K5P1EY4KYj4s3AfguAH8LwPtE5H8RkQcA/G4A5wDeLCIfFJEPAvhn+v09gZzzWwF8A8qk/Fsi8r/N1E/rvts+gdpt3l8J5T16zoc59l7CIWP/eQDe9gT7vwHAD+ec/81H1qTr416d0N8L4LkiIrPvnq/b30AxEAIAROQCwLMBvEd/9/GzfTL/fGKYL3bvR2GkL5h993yUPgFW/YIyUE8JTzQmiHl/Ief8N3POnwXgUwD8xyji9fsBXAH4lJzzQ/rvwZzzfR/tG3gqkXP+wZzz56H0SQbwPxxxGhsjaov5eJR+vpeQn+S7S5QFHIA5F8wX74dR1FUfDn8cwMtE5Bs+kkYegnt1Qv85ABOArxeRICIvB/A5uu8HAXyViHyGiGwA/HUA/yrn/A4A/wTAp4nIy5R5fB2A33Pzzb9Z5JwjgB8G8NdE5H4ReQGAvwiAfsc/DOAviMhzReQhAK+8paZ+tPBEY+IuiMhni8jnikiH8lLvAERlot8D4A0i8rF67HNF5Itu5C6eAkiJV3iJ9sMOZYGKR5zqs0Tk5foefQOKSu/nn8Km3gTeB+ATnmD/r6FIKV+iY+E1KDYY4u8C+FYR+b1qSP90EXn2bP9vAPgjKPPUn32qG/94uCcn9JzzgGLYfAWADwD4EwB+TPf93wC+GcCPojDPTwTwX+m+96Osmt+GInJ/MoB/jTIYTx1/HmVy+nUA/wJlkvt7uu97APwEgF8B8EsA/inKgnnMi/60wxONiQ+DB1D65AMoqprfAfDtuu+VAN4K4OdF5BEUA/InfXRa/lHBBsDfQJE2fhPAxwJ49RHn+Uco790HAPw3AF6ecx6fqkbeEF4P4DWqOvuy9c6c84cA/FmUifs9KO/PXEX7P6KQoZ9Acbb4XhRj6vwc70KZ1F8pN+BNJ0s19DMLKiq+G8B/nXP+qdtuz9MFIvJHAXx3zvkFT3pwwzMOIvJaAC/KOX/FbbelYYl7kqF/JBCRLxKRh1TkfDWK58K9Jio+pRCRMxH5YlVfPRfAXwHwD2+7XQ0NDYfhGTeho7iZvQ1F5PxjAF6Wc7663SbdOgTAX0URn38JxX/7W261RQ0NDQfjGa1yaWhoaDglPBMZekNDQ8NJ4kZzdXzVF356EQdUKHDI8L6sKY4e5bovuNI05wO87gx6rHdet+WzpiRZQGxf+a2dPieIfko56XflolG3U0yYpmKwH4eynabi8BGT/oYu8AIk/Q76+7/3M/927h//hHjDN78kz68T42SdkJO2a0rahglA6auu6/Ue2Oao20nbArt/ti+msi/FspPN9iEgxuX9iZ6AfeycwOnfoSvPpgtBL+V0m61N7B7vy7P6ljf8q2v3yd/+B/9nBoBhnLQtYueG9gmfb+Q1U7b2OR0nyPrsdbzkzH6MNh6wbpWw72MdJ+wTHYdO6tbZ+BXtAzYz6/dlf98FdNpvQdv5lV/6BdfuEwD46z/waxmAPaucM+B4r8vrYtbGUceN3Ycs76vK6Nn22f3k5Vic95e9W9oG9i+yvQp2OI9Zv+8iDk4/cN+rvvz3Xrtf/tmbh0WfALUPrC/sWejHlOz9TSnad/rHot3eu9l9sm8437DnxP7KmeNguU0pW//Yu6Hvz3r+EhE4HbN8Rl/yB/pr9Ulj6A0NDQ0nghtl6F0orJKroMOMydkqXVYmMnTvOzsm6B9d1+n5yHiYHVbg+PtAhl8+8xxxSvVvXf3GcShbZbdjTBgH/Z2uuN4pgyVjMYYulf0fxLf0nrRPSBDEOYiQUejqrtLBOPI+k7GaTvupz2W7H5Zsec7QR/USTiR1pBXOVbaApO1Jet/KSpwYQ6F0IMbi6jF6E3Z99zjS05PhsUc+BAAYxmj3QoYoZFv6XOtzcFUqs2e+lNJ4jnEakGJatJnsiYytnJK/w+J8lV3WZ7RmvTZmtV933ltfbjf9gT0CbXcZn5TYsgicSUXQ+yhb58W+n8je7T70XnOVbuye7d50nOe8/D5XNmr3budfPiP9hX6nF9cNe1kkwyVt6+MGbj4xhv3V6l5iZcs8SG886fs9jaPdT6JkqhJ5NCl4Nu75t0kXOq7m7bVxLovPSTNXpwzrxBBUwxDKPMZxlWdnoFTJ9pRccE+OxtAbGhoaTgQ3y9DJ7PSzQzbdFdlL35P9lbWm8wGdrmidMq+NMpxez9d5ZeMi8MreyYx4NWe6xmQr7qCr39VuBwDYD4NtHZa6yVFXck/ma3rZmU7zCIYRuhJJrCQczidjR6Z/FNup14zG2rmCk52u9e/ipOoH7XxlS1aTY0JWZtoF2im0fcr0xAc4fQ5kxySzstZdQxCVkZGFHII4lcDdZLrfbPeZ9b6pYzT9LQROrxX68uzJAtkXtR+y2ipglLZkR6h2FccR19YAACAASURBVBGx/rYnQRaszzu4mX51xWQ5FkayuhCMmSHNo8evj0ltCpONNwenkhmlCZJtSiCQek+T2lBML27DdfWOzI6xzzaeZvrhlOY/t34RwV1ZUgRLBjs/P8d5OELEjWN5dydtX0Kqz9v0/8t3JY4joM97Uul80nc/6uegY9x7Z+fhnJLNzlTaG1O2B28s25exmLkVD6/vVt/r89drEs5sERkzq59ur8fQb3RC7zclKpYdISnZi2Qql7BUdXShw9lZqV2x1cl+u9kstr2pYDwCxRi9JictTsDzbqcIy0VA7hTxLYuby54AAKfHTmyvXifnjIEGzdXEeR0Ijb+OopVbX9oWN76YcYgYR5309Jq2gJmRpw7wxMmKL7S9kPp9rEbCnHTS0S8m1Ane5eVL6QIHoF56ZogNns/k8MmLEzpfwBRTeQkBgOoGbZcZfKeIoONBLko+JS48g/6W6hSRKmrXSZ4Lo44T703XwgWbxmQuKhHZVB4cU+wh9g0XwZwCkEqfDOCzPgw0ipNczNV9pvKZqdqAonqJet91Qi+/McMtFy6RmTppZRTlQpfr+CFs8TDjo5gahoNYrJ9lcWzZmhLnyTthhUkn9MjxnitJNDUan59dM9nEPezulH2cXHWhp1oyYjaX8D3U+510O6aE+k4o6VGyGTZlLPrQIecl0YjrvqieG6a+PXSJayqXhoaGhhPBjTL00NEoWhmi1yZQtKcxwutquNlscbYtzP7+i7LdKFOneuZiu9XvO3Oz42ocyaqkroZkLGTZekg9NiVjKjSapcu0OIYQcSYVuCOCtLzr9F4oMsfKjkBmWI4NZM+dR4zqtmeuiDTmkHVVD9GUyT7Top3e85hURUxllmYQJPPwHt6bZaxsPI2ONODQvS/Ys6Ga7RDEsUhKE5n1lBD3K4Y+kZmP2iQBVGrJytq258qO6DqmfRRzrJ1KY51a60TVd5K8SVzTSJF9Kf04ZIgZUSkpqdQS/WIb0CMrM49yHEMfJzJts6ADeaVy4yPyVXoaydBXRlBKLOFxDJ4Ut6o7ZmW7NILzGub+SBHNzXmijpF6Ym3XTA22kgIOwXD1GIAZQ4/J2nqXlD4zZlIKzFNh5knHDJ/nmGqb3Mo1lNeimieiOgaIDlPRsetpZPUBHTUKqbwTme8NJUdqLqRKhd4dxtEbQ29oaGg4EdwoQ3duaahM4mbGN7fYdr7qxeme2CkTNoaun6tOvTdmTTY1KTWg/ivnaG5SVB2SuYwWPBTvckPad8qWlV1V+40zvf9Rq6PQ+FI+BhdsxabOloFUrtOt641t0rhDZk4mxM/jNAGpsJFOGXaOS4bogzMJiZ6lYpKOMk3njb2aXpRugtQ7C10fnbF2bg9Cpj5TGfowIu3Ld8bMlbFP+n1wHiEVSU34O2V8QcdLZfPJDLg5LhmjZI6B0XTWmc+BnI+MNE7W/7QFgVIUt2p3kJCMkY3xuGzNE8cFVa5uNkxJ0GeBYPze3O3MiKc00lw/y8eIVE2XFGiNWtNdVKpLXlruIzPOkKqvtoAdHXt86UyCjlVSCIe/QcPVowCqPjulXIP/+EyN+dKAnswYSnuISW+qS9/td9r+bM4Y1Bqwv0a9ZhRngh7fXccgMrodiiCMZZ4aezJ02inKIeYq6X0NgjtQamkMvaGhoeFEcCsMnfrajAyqZSszZ2ARw/sF3twSlyHdDKVmMFLOaRZyuwz3ZVhxzIIQ6BEA/a5s931ZTbspIUaulmWnCgGQQCWZ/nbGCIzWHAD2ibnYIVWj93oFJ1N3QDexf9QtMNODQbds0zig8+W7fqPXGpVZqBtc6HpjR04DqJIpY9Wm4VxlFGs3HMdgLral6Ax57sNB6UMZdRrhE4NC1M3sSgNK1MMo+2BePGGz1fPQK6gcS/biQ4CkpeSQTb9M98gJoufWIWBjFaZbH8w+QXZKZxzPsZl0G7K5TuQjxglQvVz4bLxUdTWlOfNSQU1JUBm6sse8lLBqoEyyYBlnbVyy8BIKX/puuis4q7Lvml6C+uyVp1aszNikm/FwfrlTHTqfbk7AXsfILDuH7QPUG20V4k+Pqh1dmHfq8RYTJk+36blbYWXoaSYqVSuaSi0zt19GeGVXPXKAOg8lSryT2IlcY+gNDQ0Nz0zcKEMn06SVOMzCoemdQpbHBEadc+arPemq3+Xl+Ri6n7KH12U5rnWjc8UjPVdWYbpkHhnO2LBXXf5my9VzmTQnx4hEq/qBFmmg+q0mZcsxRqMNpvMErer0BY6mI+/60q6JrJt6YmUBXVc5g1MTvO806Kdncq1swUwJynjNE0bZdy9IlnhInw1101gG4IhzxtDFHT7EBOV5em0L8oiRwUYDPVmUQdETZnSWtoB2GU+drkmBpZ09NoCGv1P6I1tisBnGAY7RXjP/ZQAWxp72V9XLiCH42o9xWjJ0FzJcT7Z+3GtHf3pTnMtMV20O6dR1h9mxy2MsLYaFoOsWuaZWoE1mRW+dOOsO/o6fzSspZXh6tzBYJi0ZOwO7iiRJxnp4xcNpUO8Uvd40peo1skrMxzlhnKbqvWNBZ0yvMSy2SFXancaZZ45eSw+p/ueWcqT8nF5OItX+Q1HPvG7MfsO+SrM8DYd5zt3ohF6jqfjiexNRzKAmVCvopNEF8JFQxKOK446KR2PUfCZ9X/OfrEQ+ugCGEKpRR48d1S2NGRUzpBr6HF9qaLv0haW7mNR8NHKEKF2Nh1R5OBP7OSgYt8FMimnawzPASSg6qlql40RcfpM6Z4Y5Gn74QnOCKxlBVIw2lyxO9tWQ5s1NUScmm1j0/AxC8p2pgmxiOQBxvKPbMkHs71xhvCoT+bRjQhp9afU5IAmy2hpprKM7HtV3mVG1Y7TOpfGKxEIYiToOSHTDtMdKksDxMmJUtzdRlVW3UbUTZV9GKY4DQldOtOkOj54t111P3pVE8FGYitEms5qDJ5haQCdtczagUT+BdslgAX40LOrt5GT9awTNolKp0ptqLhiqJSy4bTl553kg0BGaqFFVb3QbTLmehwFA6wl9SMkCF8UWf70F7ZueaqrOVTUGFyPORzRcxmSqEZ6HizYjugXVXXWw95sEcqlajdNkpGE4cJFrKpeGhoaGE8GNMnQxNUZ1XzRWq0slV1FTneRs+2i8GVa5ySd1Q9xPozF7rsqThYrrytvNcpHoKk13RWb3G2MyaYBSkgk+5v7IbZqJ0Ievj9X1i+eTmmNjZdWxkONpggtUkajkoewvMDuiWvK8OHONpBRAt8+aRdCbeyK1DIzjudpVCSebWkZZl6p32J80BIrz9VmHIwKL1GVsf0e3+52ppNY5Z8iJZEjWeCYzPKNqSPtk2lOtNVVmO3aLtnuqmmKqaoE8y1EC2GN2wVt80jRzOQOAxMAuMkAHZBpc6TZ4IKrhVqWzHKqrG1m2ZccsvwmdN6ObDauF9FvdBb1Uhu6kju85UnI2cMiKmVvGXGXhTGNg+cZXfUcJIMVskvIRGkskquIoWcY805BxLlkGQAHV9bN+ZoOZg38WdGUdVzaR7yNdGXIs/TLbaQFndKFNo0n3036pok0rhp6mqdZoiI2hNzQ0NDwjcbNG0VUVj0J5qKNldj5tEt35ssAxRJasfVqGWe8HBjYMxj7J1OlaROPHhAwfl/o8GsL2et5hjBhUN1qzGdKQscxu5+DqsniEEjAL0w9QH1ellnUVobkNN1IHGeknx1Bh/ahq2hBqRRgGDzGfPG0GgIdo4iAyjIkGRgvS8DXn9iLFWdUb9tRLh976yfnD9cXBUe+oRlyJcH5pxAzMM81ArzECKjHw+XYMLErVha/cW0QaaSwv10yrPNiYJkvUZJkYV8bFjISOnrTmUTsLwAGQHdlrzcOfcRxDp3stWXNMEZ6BX6u0E2TGDt505FXfTqO2uuN1dBWuNgnqvteGzzhNZp+iFEZ3SDZQgjMHhmTsv1ybiezYlxl5VjHs8Pdnd3mp54G2c36OlWHW3qeacqtWG6PUWe0jAJD7MAuILGByNEug5efGY9p29JiJksNkDN3bnMfsmUsHDqTqfj2lxtAbGhoanpG4VYYeU2UN5vSfTfkHAJDgLVyYIfpc/WuABHVQ2XSX3UbDrEm4VG85DNFSnpLpWNKjiekws3l7cBWmC6ElAfVVb26Ji45IzuU0vWxk7dLkLFSZzJB2BdNd52ReO2xGoreLunZ4sq9NDaSqlZCWaRZiEotsimY7UIZhebYF9OgjozBepGwG2veb4E3CwhGh/16ljc1GJQoERKYmUHkg+CKJ8fnGMSLT9dMSLqkLJ3W0dL3LNXUy9eo1l3vVs1oNVb2XUc83qN52zBPyhsFter8WKaf9xjz/G4HfKFt1h7Euoo6v6sHFMeHNtUObQakM0ZrE1BmhozdSXmy7uYej5QCgp0/1DNvTw4hjwzyt5pKzsm4mgFu54dHLJea5Dv5wfvnof3i/npfutX21uem8MVICoK0sxsquWW2L44h2Cm3T6KqtgWx+2qvrrLahJK3j3KSeWbvlu5ZShOe9O0rgy/z0lA5yrlLL2HToDQ0NDc9M3LCXyzzYAQDE/IHNyszaliwg4T321H0xvS3Dmqm9pQ9qAnIi41ednZ52sNj6CaEWQdTz6Udj3XkWAt3podRbrlhSznd5DxwC6pqTtneaavpPruAD08Im9XnOEzr1cpGOHgus1qPMTNlgd+bRdzPHZVQ9o7V2zIh76jG1WAclG/NkqDYGsngW4CE72mlwU3KTsZojugTbDZmxjoE+IPUMbNF0w1rpaa+NGHIdF9586rWdJlFU2w2ZDz2b3ETvkJm3Cn359di9MvSdBjft4w4ORVLw2sfeCrYyUK58DH1Nq7CoRXkAKBlR0grINobNr9pS2bLtgp5jRNMn99Sl6zir/kJi1bGqKUFZ5OxBMo1yjJrgivp6+p4jmW0n6XYtBVvRkSyloAxwVEqEy0c/UK7tWMVsU6tkcW6xYibKomcpce3afDfYtxajkaukbH2tp+f48t6mNIZF0FtKLJXEaO+CJU7jvEa9O217Y8R+oDR4mL2lMfSGhoaGE8HNJufSldPPajXWwhOMwiqwGoGSqy849dnK9FlRm3ovcd5Y9k4Z70imzjSmOdakXIziYjVuZbnOBYuKJP+20PxU9Vx6M7ZiHxMpmmgFV6+L/X5CYvpYRz2aMl/zq81V50ePGHpwMBqXZfU6D2yYPEujP7Ujh51eJ1eGF9XvfFSqMZhOPRurGrXNrI1IbwmLvJ2ykeEuHN4nZ5rOYFTdbHYO0cp/0W6iz5zV7QVGT/h8RzPMLIsFTHHCXpkPWXfVHSvLDw5Oi4hMC+8IYGDf5BGdpY1gal7aJzT9c88kZc7sHf5ws0K5L30nWIQkp2wO1Sxa4qjP5mckdPpd0HY79ge9Xyz8Heh7i1kvG7JKy0wm8CpVTjPbFVAjkWOK5msvljLA7kJ/Q2mvjo+lx/v1cPVoSZ/bdZo6eRPh+/I3JTZLn0sPsFgLuvCdpURpqRPc8rflOx3nTKdhNVSzHRe6Zf3jbKUrM4I9q/JzjqtJJT5LKbHbWVroMR7G0G84lwtfvpkrFF8gtzQq8QV2PtgLIDORGahuXH7W+clc55ivQt0YfRV9JhrNZCnCmujpg9n00lqkrWkWy/4sNd3AOlrhGqA2wKompVothS/lbqR6pxzc+Wx5nDn3MyKEGeHYf5OEmq/ELyf0rJPtGCdMzKan1aFEDX+wIC6xyZ0BWVRjJVW10EHReW/qsSO6BJuOfa3nl2x+c1mYE1zHgKY8GBGroY2qCX0ZGPJO17lpitipYeuxS60paVFWuvh1fdXQWKUm3r+edxMg/dI90XLhcDKxSlAZmy0DeQ7vk3Ky5SSTc6pEiAF0OiC8LoqYqutcYmi+PpMuaFZKczaYsCPxIZGhaytTcngHczUemXaBQTh8j7KNR/PmYzZKchWSMxFLGZGPcFscdzoZKnHbuoCkuYAsu+uqOH0SV9NCsAB9t8xrUwOr8kwtzB/xGAb/THV8rlSyJEF52NuCklbVshzrmzKT6H5vNU6n8bDc+U3l0tDQ0HAiuGG3xWUyp4xsfzOgKDAEW41e3bavecGtkr2eRrde1Qs51+AE0VWZ2QK3G2VX44D9ncLKxNz3ynlorBDvLWiJGeToTskscsxpPA1DDdU/uEY3wIRGjgZACYiJrk56DdYujGRSqbI13XfGZFhkIzQudxv4XtUvyt4ntRQL+xrJRLukTNMr8/Rk5WMy1ReHjWk0lKkxN7jzteKTJW46AOdndAWkWykgVL8I+50qIWUwIWFUKWyn++KVBoWRECnbHKdoaqxLDUwZeQ9WQzXg7Exr1eqW0oEP5bf3bS4scMbRXbTjZ+g9aPN8STsBANvNca9d9RYtbfVOTA/JqkoM8ukotcYR0Vgx1Qlq6DejnLLJYTCVE1MAMLDONIze13FJlSXzqlvcVQbf0pEBOiMNfuV5WU7+HIyhHyPNRVVXWLK1NEEiJTLWc4XeE1WD0dQbGwZZ6bGsjrbV/BExVqcHc1Pku8UEWvt9VcvZQ9J5gu7X02jZP5MFMWnqDEredIZIqQZBDRTBr4fG0BsaGhpOBLeSnIvrSJJsFIYGU+q+WQey357PQtVZz1MZAZNBzXI/k8VS52fVXagHTdHcmrLqrHoq1CxVZa6BJnQrBPOM0x2LVVaihejLEQzdDLQM6xZnCbLIgJ0+Jhp4Y06m1+29BiZpCPgwlXb3YNKpM7vfnkxdli5k4maukcp46K5It6mEYHp2Gv4YzFIjlmsYtZgO9fA+YQpgKyLvxdIM0+4xqgHO0V1wK6XSC6or4l51qZQgRjUC37nambTBUPSJx5hroOABuR8AcKGM3KnUcsY88CHb9Vk4SgVLbM8097oapDe9YKt/n2+Ps4qaey3d2yaBdDTq6XhgfQHm6E8Z055pIlS3nNXl02xJ1TbFsHQmjbV3ZZYfn2mN6YZHDmu1PFOyPmcQDnXdo7WFSvRQA3bSMU4Fmu6BNqAUISMlGM3SpsYeK1IlVYLISfOpk7HrO3Jm+fxzra+7Gsu0HQFiRtDJqjnR5mOdZG3lWZi+gtfme9WLw6AH+QMpd2PoDQ0NDSeCm2XoXJE8U5UmdKrzDboykhEw7aoLG9NZZUfPFS1osS26zWX9SnVlZDJ7Jl2iy5Ik9Pr7qNVOmGCJOsaUYg0X96rDknLsQDbChDqA6TGP0RezuMZo1V4SuM4ysRVXcGMcOZmlfJroisdk+fTqOdN2d/Buo/fHQiDLoJdhmNAxHJyukspyhomeSaF6eSTqHZdBPp25fTrz/PGHUgwAvQYWMbgsZQffq3QQKLYwfL1cZ3cFqOoUI/Xsur2jTPQyl2d4J+4xKoMarOKQskTW3YRApmJrYXK4i63247k+n62D39I7olybgV1kzExf0PeCje7b9MfxqHVaWMRsLJHsOLEAKm0zs3j+ThvZrwKK0uy9NBfetTufuXWKJdaKjlIcGfYswRXdJunOsm57niVMm7HYQ3FGiZ5eYvtYdd4qiWYmGKP3GGqiOkssxvqxnC/UbbAkt+N4XM4to95Ll7PZ6tShCKMFFKmr6zCaK6LVhbEHqv05zFJEM41vK3DR0NDQ8MzEjTL0vbJR6um6PlgN0V6DAcjyup6V2535f5IJnl2cl89a3Z2M3fvOlj8Sgb3q8Cxc2gO9rriDp0VfWQkDNuJkVuu8YvwWjOSoaw2Iql8/gqCbhwHhvceZMkE+nmGgjl5Zl3Q1pfBI7wZa3lWZywrzk0PWv6Mq5qgLNKeJ6O2YnaYA2GuAEbdZspWV86q3Ny8CSkxUIIurDO2IYKvAhE5Sg5rU4cWYb7aUuJTattVdqaNriT5nlWYeGehz7o3JdNpvk3kc6Hj0AZttkXbOn13G20PPKjr1szOOWW/SHpneVn3NKWUw7H7T+5quQQ5jXYa7ilnEyuAsbWu1FQEAYjTfatbpZfEFhuxbBg1UH3P61Xv2pf52miaTDqnLHxgEo9uUUi0WM9BXfRlbYdk3YqzeW+lw2wJTA8NSOQyWHsI5tRmsStHlnBH1d1u6Hmn7Ll3xekra/r7v4DxTVJdj6bkz0j4wDnZ2Bt0x/fCoNqmrx65wtaN3HfR3OuYsZQKlmlqkw+IArokbndApxtPNMPjOijCviwozCnGz2ZjYttEJ/Py8vGCc9DmhiPNmXKTBr6fR1Z5mRM/ON6sOJx9VuTiHoDMIqwTtdPJiABQz5om4mRvXdHCfMIMe3Qx7l5mGG7BIt3Kfoxq1pphmBly9X007OGpu+P2VitkBiD2DQjggYz0PgHEY8eiHysC7fHRcnI9qnmkS9PqMOs+IXz7HWoGq/F+LbOOICd0ySFKdgBq5Rw0O1UVUzYVtB3+mKqmt5hjhXKTGyKTrzW6/r7VENZLT8t6TbPhgAUn33V/UVw89eB+AqrISpFriibnA9ZretkokXLLsluGI6Flg5tZn2QQrUaGegZONxKJeSkGMINAOubPgw+XYg+8sGCfTHXNlCMzI1lfjyvA57RgEI2YUZWFkqzgVqe5hFO6EqGPtmFwuxI6LSZyAxEAdnW8YEWv9FhA7ut5ykdQJXe+l70ut0s1mY9k2me+FwUIkAeM4GkkaLR8LM3+WYy4vL3Gl9U85NugSTXLRaz3azcbXWsgtH3pDQ0PDMxM3bBRV1QaNmCGYSxCZuVcjaaeuYZtNb+xsu1HW3lX2Xn5bV+DJ8iXQkEimWdqQ0mT5iCdPVrvKGufE8qo7xzwL5fc1PHmWgyItDYmHgKs+2UMWZ/ezNiCRGTsBYqLYp654DOqhWoaGrxwtBL4PFO0K2Ff7XcTVpbr67Zimj+ehe6SbtYNSVGGurK9KBinia8DLEWoo5iNxVS6HqIqFKg5KSnQL8zmjO6fvoLLsLdVS6pJ2vxqukLG9UBXfZpnrmswxDdGMVFtlThdnqmpiPcxpxO4xrZgzLHOpuFoKqeyPRXIBYFkOD4Wp9GYViCim11ztyh5VDTi6bG6GdFv1lklwyYid9zXfkl6DhryqGMz2LtAQy0C93ZVKBRBEBtbw3HopqlRNfBqljv0jjKIMRLy6LOx32I+WBuHObrc8NjDYsDdps98s8yZNKh5vGVS2Pa9jje+5HkOGHsfJpB+6d9IYyoyp+6sd9sPSKDoxD4LOO+dZ89G4fqaVOKziV2PoDQ0NDSeCGw79160ZZZKtymQLzMRobm8uGEOnXtYy2bFeJZN1pVTDj42ZL3WvCQ6RldkjjUsMBqiBAs7V6wMwXX8IdDOc7LfZ9JeHG7vITpgXHVKZKfWxzMe8kbKCD6NYZ1p+dqUI6olpIdZ5BPaXylA3ZGhkT+U3V1c77O6o+95I6eD/b+9K1hvncSBISrITJ+l5/9ec6ba1kJwDUQVRfYl9yMynH3VRFi+SLFMFoFDQ984oJidGDqhvgPGAodt0ooFM15rJvo+i0UfWyKKWSu97SF/TgOId2s6LZJy3z7Zfv/51a+dkbq9zedPP9DLI9QZrCTBGRAMatfyeZfnTGBQKb2DovF6WVSLytZgchNx57KWBUivztS/1uIs1TMFre5TC4rwgRw35q5qWyWARARjlNKIGArM2qwdNk0apZdT33NVtpLX5Q9QA4QCKg5wYJYFWGZAyYi5B4ZQs/b3UvyTGzwCMP8TG0GuoslVEDrMeAxqN7Lg5vWnANCrNweu5etM63XT9Q8sSRHHMb6OFP2eKG2gfgohez9FjfjBnDokw5thuS9v3rFH2OH7KTa/Pj8kZusPhcPwj8aMMfdGc5OViRjaoeqP1Og/KyjaVJJRqU2Rgv5t72dTAyeR1n8ht/0NXs+5DrZYXTpA4krFnPoh3bjQ+wZpgM2Yu0ox68HM95CS/A0QQ224OYpWRP4tYtAHlSZQgKyyIBc1GUDfoY5WN/Xt9yDQpE1CDMnhbQwn0536Xh0ofytEsjL70kTa8kE8iJ3y0QI5xpNQU1qXPANNusE0STWoJn20qIrS5Y4xknpCpwZs6Ca6TxrqGS5JR5YXX25v+T6MWZcG/hyibyhOpQhjsMxcRKWuQurbr9IGGJHiWY15ntGuKx/UaQWcz1IB8bzEJYtEoJGJOJYzEQqA+lSou/dxhMkb7qVppuDVOYMtHuW6y6HCDlXFv1bDmKsvaN99kWGjouVv0+ngsQe56XT+250/MhWq49jku829Gz/cZE6bU7jjieBfW8SBbZGMWJIVqqyz3B42/GLUiw6D7EIJNXYJnPqIYSEWXNcu2QH1jtSERax4C468iMup+TZfn1hRn6A6Hw3ES/ChD3w5zHMcty3Rg6GXqZxqWnGWDNnjo2QIsVPsBBlplhuEPp2ajqcM0zcijbTTxR7OKUKVxNMlP+p4hYtpPEOrFX5hwD9XMwAaVKBgjCJk8agZoh1mK0EaWihhMyqGap/19nu/M3a2qYIm7JhGRdu5hJjRMOAf9vT7WQS46EGFMiFo0h04DNGXjIdkUnBdkLpsO18AEnBBGo0eY2qTqBubNw8CmI+FcUGWyEQy9/f36PnGQyhunOSlD17/XdZCMmaL6gVz13KAdfglZbmrCFTNYO5gxjl+j0a3yEDCd6llA4zwiwiqBNSfYaYSKvLjmpZeMHjM29SAyG8Y+mo0xMoe8bKpYAXNFvrjYdwvqrsBeduTNA03OUONZwUILjNP0HGaRWZn58gJDf1P7j7uqVe7jbENyKtYA7Aui65GTmAIj+l6Vszeu43Qs/fwYHOJaCTtt+mF+KQbOlG0xc0HY5errvl+h7MNc4EEmte+93Z7Lof/sCDqOHtovQiiexf4xDLGr4DQzHNTQqURN06DIV+tO6tc2HOyKFMw0MuZlPTFD+oVht0Gq9DcfFv4O/tM5V3MZfKEvAq6Ogy6GeV2seAy/i4gbWrt485YZtkcU34J6uOhCMmOsVchMaz3mdr6Qmig8pkGmEVJESKe0g7KgKDrJNKKRC/KvhF1A5gAAC09JREFUfgKSNRNFG/v1wklBdx1C5zFVdv/hxopRaQjpJYlELQS+6RcEX6rc9z3JlKpsusDEbe7+iSbOoW4S9ZfMsW6ahoD3eN0YxmOx3yAXZeEMDUvtSy0i9CR6Fn8eGBIOn+8oAz149PoRjNQz/xIboqNkSReWpMfKgd6psojJEYt0UtTvTLEuRss5tI2lXoJseheZ4WKJ1JBus5KzOSdZ4KuTn1/Q39/bNfl4YGRbEVWQykMXZQw33zhFK8uqf3ugQxg3RGxxk9vJNHFDRBolMc1SdwXd/hgwALpK4U0ED4GjLDrfLyqlvb6/y0VvVFd2jX8PnnJxOByOk+BnG4t4/7AWXDQ5mFyxb9MNEnkntFmKeqdl6zrSH8VkfGSw+lwWPgvDRzQRMIUDhh53TRPazpzpZazFroAmm2TvEV65P9LzjseIgk0hK5Lu9xhHDl/mUGAUwfTlsoavOQqn/aAZCRJMGPOVZEVgMnTIA8GaYhI4+EFiac09/WcmIfJ8xxdkiwiDIyOfzKgAKSXMr2ToHCvleUMA+9PjBUtGZLgtZKt51mgPg2FwTtaVxUQUHjeNcNCYFkQkoBgawMSVqeE96bpX2OAEr5pnMa99w1WUQLnqBd5C6HBBWidnXrNoNoOUDs6QV6QKS7FUJ879kXHuZIZ48D6tJNJSLphCRJ9/ts+rb79GB3Ouctfv3Sspl5uyW3iobDXIXffvTd8EtuUo/G+lMppAqiQglccsFOSHkbJVXJeQKI6YJVx3UToaGsHwUQyOSYbDrFxGV0y1aCH0epHLWyvyXt6coTscDsc/Ej/K0AFjnoUFiwl/gxQKfuPJCnRoOaeDH5g6miqkUDYk9HhGUwCkkpm3SBYFdVs4eShJXsHa9X98jG7ho1133s8vmAvhGcjjpzSYLA7FJ8r40GIfzG0QhZldjlNEZFBHxHgdWMxZIwq6eHMtZtbKhphRmTDmmsJvfJgm7pcZqmECkv6d/uWVcsxXOAPYUti1URdlwMLinzIhPKkWgbU1JqZX5DU5ZV3P0Zz5+c1r39rOKHCrvAblwFajOb1xpmVWaZx5XGNnUJQvUhA1ra+ZUP1HW+u3DYW2CIcGWdXWAT7mmQy9mg0ErRT0OJTBPqrWW2JkDYs+9vDq3jXd8fvLSBl1G0x9qpTxBRTKudK0xz40WvyzBvn9yHzes/j4bA6YyL8/1k1ukC3qdUBvcnx+uewiB7wSrDcQkaKwvpt2phtc25gjW3f7jfMGyxEsCeM48HkowEN+OmmeHM1Mn18f8vHRmuJuN7jOfg/O0B0Oh+Mk+OHWfyhXkHPLbHIAI8bk9vneWEMKA410wJbBmNj2Tema7Bh6L18069FA1oH8GedKohq+bjQ72ihHMolfeyzmOhay41cmruCuTXVONCaBhilwI1qZFpOVBXDUYKxRumcl3rZhFYtHkHnUyjwhWDbqHVk7LUI04yYwc5Mpgs3h5YIpX16IWhgxbRZ5lYJ8reh+KTuEOiUF5u3Z5JF7eV3cKamYT0ZelFGB6HtX1lbgl4+gIy+oSWTmozkXkqN9kG/VHHsVY/qvOJaJyEyLWCgwEjthtg0MXb9b+K7kLBGyO0h6GYmq6qmgDV4kbJDDmnKsbfWwJEDoxahyXmAju2ParKfoIcPjXiWks0aA91XkseJcPX9O3j8aQ3/oZzIvRe4aeXziHOi+XEbYRxcpNASzPHj7n54/XDMpUQG2n3gkYvWhnDNtjNEkRBtmfe4QAw3KoNB61/z4r692DL9+3XT7JZ9fzar56uZcDofD8c/E/8acaydkPRre5IzctTHkVVkeWmetMahvCpFaOO+QioZjDjAmCvvBsnFXXpkvX5lPR3NCZqNS6V4v50IG+ApDB9DeX2o1whuYtGuPqchVF4m6z/VQTYeoBOcvh8yK+6QMHecIx5hCkGHszYpyhd2AnpOQJCojTwemjucwp7pmY9DheTZKa4fdFPki/TkGW2Uz2BBZL4nUebff8bma8ib8NcZy1WhgiKaGwvVWaBSHesWuae1g6hWYV4Ztruqch2Da8FdGW4ldi1CyPNbA/O2sbDupDr2s6tJWs+03WsuVRWLgBhpcUi48I8bMKfvQjfUYoNEJmm5GT9kabPj5I2LBFCo9F1uJVLy84ln2cWtMdp61HrBk5tBxDcK248+kNYhcabh1HMDCKB2DOFIyJr6vw+1QS6HqDI1TiK6vF6izAq8bfA+/PlvO/Evz5Z8f7fePjxuH+OD534UzdIfD4TgJflblwtZ4y3FyLF3o85Sb3uEe97tk6sz12cosaNgPq9sULPfXdZruGHqIzFGbiqT9Cwb0edv4R2sf7tnIBgFEtfeSFzTXdddS3V5i4IHmw/4hoqglSA3oDOz3b1I2Ml5gVJUtZ4scM3O4ykZroXlSVvaH94bGPKVRAlQtEYyc2cT2WGjpk6mEwgthizXU7fPcUEEpEzrMRc1bsK7bEYZLyiSpghLd/8CICIoVnlv0FEiVqr0HFdPtOcRC969km/lYe1VDVvUMBRI1cd5nsRmDzwG1AdSAtkzLVQjpAwT12YYn2KTF0m1x7MMD/QlxZ3qG98SWP+z6QPrctzH0yu8HPidTj/S1ha0mMvSXBlxofQ0WALfbykgRzPqq0dzthm7SbPa9+p6sZSHg4jsY5zV1ntl/t2PcRbuIpmGnrPswpsjPATl55NBv701z/klly4dc9XjA5r+LH13QeXCYYRgCT8SqX1R5oAlCpVm5ijVna6Hh4JBmmQlzPSuHYo75TgS7aBkta1PCau3JaCRCOoaTYTBPcef/ghb49MLZrLQSMO8IJqQONTQWcCQIejDiIbWE5gzKzkLkArwwVIR8auAbrLCZq1jQe4+ZGhLDVNogcDBub7cgofmnt8d+90zsUPsvTFt38B5o8MIXB88x2wgU2vFfFECRIogSKYWFgyeHLRe7ndDTnJcL9kt/L9mkgHgWi4coyO0XSKTHXsvNmU8IGuLMjqCqhUHNutWFPZZiAgGSh6MMExLVaH7e9Dax741IK46X3L+Okaf2qqUKG4v4P5zniFThwG0J/fX0DCD5u+EVUpI3XSC/frViI4q2IGzzsjAdYxmlXv67HzaP75Z9lL0AIbTG/u75vHZx04xBRnX/NJsNbf3XY/jQlMs0TZzMhe134SkXh8PhOAl+lKEXhjOWDlkrGnRQZISMT4s988K7Du6i46GAR8TQyxNFeKfdkyI6CZKp94ywlLIrmIJptQ1MeNCQknOmgdUrdJRMMZKG79iRdFvaIcTAtmE2fiAi0R3Nu3QI2CxZE0LuCr/1ffG4LyoXTctEGcjaEwtkmoqAZUICA7Vo6q8Q/huA0xw/QhFLaUQ7B/sHVbE0Cpkwp+NocXSzwrM5WaIRRPR1hD8kWB3on8CsWTCLQVI4RiL6OUBtptfUOCb6s2MO5rPY1t6FcpPComuGOyIMwApMusqO9+q+8bz2Mrz9R9Wde9kz2ciIlmm5at/n9i7BojkJ3VY0WgwRhfVJIufEPp+yvH20oiia3sbpIrfcS4tXzFndYDGwMV1kRV8cZ5+Kk7qPTvpr2iKdHcOv/TmNu3OKa22gmEMZu6ZJUQCNQzIXzSe/P87QHQ6H4yQI9aUkp8PhcDj+3+AM3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE4CX9AdDofjJPAF3eFwOE6C/wLelhT84KAW6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
